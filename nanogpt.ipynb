{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddeaaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c951a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"N/A\")\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08afc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using CUDA')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff01cbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7cc4defdddb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43875bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tiny-shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7cde1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef510ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92650a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(str().join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a123c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7f42f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786f9d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db54d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s: str) -> typing.List[int]:\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ints: typing.List[int]) -> str:\n",
    "    return str().join(itos[i] for i in ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9ee756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(encode('hello world'))\n",
    "print(decode(encode('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7aa338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = encode(text)\n",
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03251ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115393])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encoded_text, dtype=torch.long, device=device)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596a940e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf5a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
       "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
       "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
       "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
       "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
       "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
       "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
       "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
       "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
       "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
       "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
       "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
       "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
       "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
       "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
       "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
       "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
       "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
       "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
       "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
       "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
       "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
       "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
       "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
       "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
       "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
       "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
       "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
       "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
       "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
       "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
       "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
       "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
       "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
       "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
       "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
       "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
       "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
       "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fccf692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 1003853, Validation size: 111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f'Training size: {len(train_data)}, Validation size: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da62e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # Also called \"context length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e474082c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c083096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- As characters ---\n",
      "When the input is F the next character is i\n",
      "When the input is Fi the next character is r\n",
      "When the input is Fir the next character is s\n",
      "When the input is Firs the next character is t\n",
      "When the input is First the next character is  \n",
      "When the input is First  the next character is C\n",
      "When the input is First C the next character is i\n",
      "When the input is First Ci the next character is t\n",
      "--- Encoded ---\n",
      "When the input is tensor([18], device='cuda:0') the next character is 47\n",
      "When the input is tensor([18, 47], device='cuda:0') the next character is 56\n",
      "When the input is tensor([18, 47, 56], device='cuda:0') the next character is 57\n",
      "When the input is tensor([18, 47, 56, 57], device='cuda:0') the next character is 58\n",
      "When the input is tensor([18, 47, 56, 57, 58], device='cuda:0') the next character is 1\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1], device='cuda:0') the next character is 15\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15], device='cuda:0') the next character is 47\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47], device='cuda:0') the next character is 58\n"
     ]
    }
   ],
   "source": [
    "xb = train_data[:block_size]\n",
    "yb = train_data[1:block_size+1]\n",
    "print('--- As characters ---')\n",
    "for t in range(block_size):\n",
    "    context = xb[:t+1]\n",
    "    target = yb[t]\n",
    "    print(f'When the input is {decode(context.tolist())} the next character is {itos[target.item()]}')\n",
    "print('--- Encoded ---')\n",
    "for t in range(block_size):\n",
    "    context = xb[:t+1]\n",
    "    target = yb[t]\n",
    "    print(f'When the input is {context} the next character is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d6de2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset: Tensor, batch_size: int, block_size: int, device=None) -> typing.Tuple[Tensor, Tensor]:\n",
    "    '''\n",
    "    Gets a batch of `batch_size` examples from `dataset`. Each example will\n",
    "    consist of `block_size` characters. The inputs and labels will both be\n",
    "    returned, both of which will be of size `(batch_size, block_size)`.\n",
    "    '''\n",
    "\n",
    "    ix = torch.randint(low=0, high=len(dataset)-block_size, size=(batch_size,), device=device)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ebe5d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[35, 56, 43, 52, 41, 46,  1, 59],\n",
      "        [56, 50, 47, 49, 43,  1, 44, 39],\n",
      "        [13,  1, 50, 47, 58, 58, 50, 43],\n",
      "        [51,  6,  1, 47, 44,  1, 51, 63]], device='cuda:0')\n",
      "torch.Size([4, 8])\n",
      "tensor([[56, 43, 52, 41, 46,  1, 59, 54],\n",
      "        [50, 47, 49, 43,  1, 44, 39, 58],\n",
      "        [ 1, 50, 47, 58, 58, 50, 43,  1],\n",
      "        [ 6,  1, 47, 44,  1, 51, 63,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a43b7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Block 0: When the input is tensor([35], device='cuda:0') the next character is 56\n",
      "Block 1: When the input is tensor([35, 56], device='cuda:0') the next character is 43\n",
      "Block 2: When the input is tensor([35, 56, 43], device='cuda:0') the next character is 52\n",
      "Block 3: When the input is tensor([35, 56, 43, 52], device='cuda:0') the next character is 41\n",
      "Block 4: When the input is tensor([35, 56, 43, 52, 41], device='cuda:0') the next character is 46\n",
      "Block 5: When the input is tensor([35, 56, 43, 52, 41, 46], device='cuda:0') the next character is 1\n",
      "Block 6: When the input is tensor([35, 56, 43, 52, 41, 46,  1], device='cuda:0') the next character is 59\n",
      "Block 7: When the input is tensor([35, 56, 43, 52, 41, 46,  1, 59], device='cuda:0') the next character is 54\n",
      "Example 1\n",
      "Block 0: When the input is tensor([56], device='cuda:0') the next character is 50\n",
      "Block 1: When the input is tensor([56, 50], device='cuda:0') the next character is 47\n",
      "Block 2: When the input is tensor([56, 50, 47], device='cuda:0') the next character is 49\n",
      "Block 3: When the input is tensor([56, 50, 47, 49], device='cuda:0') the next character is 43\n",
      "Block 4: When the input is tensor([56, 50, 47, 49, 43], device='cuda:0') the next character is 1\n",
      "Block 5: When the input is tensor([56, 50, 47, 49, 43,  1], device='cuda:0') the next character is 44\n",
      "Block 6: When the input is tensor([56, 50, 47, 49, 43,  1, 44], device='cuda:0') the next character is 39\n",
      "Block 7: When the input is tensor([56, 50, 47, 49, 43,  1, 44, 39], device='cuda:0') the next character is 58\n",
      "Example 2\n",
      "Block 0: When the input is tensor([13], device='cuda:0') the next character is 1\n",
      "Block 1: When the input is tensor([13,  1], device='cuda:0') the next character is 50\n",
      "Block 2: When the input is tensor([13,  1, 50], device='cuda:0') the next character is 47\n",
      "Block 3: When the input is tensor([13,  1, 50, 47], device='cuda:0') the next character is 58\n",
      "Block 4: When the input is tensor([13,  1, 50, 47, 58], device='cuda:0') the next character is 58\n",
      "Block 5: When the input is tensor([13,  1, 50, 47, 58, 58], device='cuda:0') the next character is 50\n",
      "Block 6: When the input is tensor([13,  1, 50, 47, 58, 58, 50], device='cuda:0') the next character is 43\n",
      "Block 7: When the input is tensor([13,  1, 50, 47, 58, 58, 50, 43], device='cuda:0') the next character is 1\n",
      "Example 3\n",
      "Block 0: When the input is tensor([51], device='cuda:0') the next character is 6\n",
      "Block 1: When the input is tensor([51,  6], device='cuda:0') the next character is 1\n",
      "Block 2: When the input is tensor([51,  6,  1], device='cuda:0') the next character is 47\n",
      "Block 3: When the input is tensor([51,  6,  1, 47], device='cuda:0') the next character is 44\n",
      "Block 4: When the input is tensor([51,  6,  1, 47, 44], device='cuda:0') the next character is 1\n",
      "Block 5: When the input is tensor([51,  6,  1, 47, 44,  1], device='cuda:0') the next character is 51\n",
      "Block 6: When the input is tensor([51,  6,  1, 47, 44,  1, 51], device='cuda:0') the next character is 63\n",
      "Block 7: When the input is tensor([51,  6,  1, 47, 44,  1, 51, 63], device='cuda:0') the next character is 1\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    print(f'Example {b}')\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'Block {t}: When the input is {context} the next character is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1034574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    token_embedding_table: nn.Embedding\n",
    "\n",
    "    def __init__(self, vocab_size: int, device=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        #\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the vocabulary size in this case).\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "292fb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5682, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size, device=device)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "822c6f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fCxBDkL-k\n",
      "zc.wfNZHxO Fn,yRtK\n",
      "axxP;CkPBbABXGeCXSvgO-3 SMmd?Ya3a\n",
      "hX:Y?XLtp&jjuHqUo,Kv.tbyr dXp!FZaLeWj\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b039cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: BigramLanguageModel, train_dataset: Tensor, val_dataset: Tensor, eval_iterations: int, batch_size: int, block_size: int, device = None) -> typing.Dict[str, torch.types.Number]:\n",
    "    dataset_splits = {'train': train_dataset, 'val': val_dataset}\n",
    "    out = dict()\n",
    "    for split_name, split_dataset in dataset_splits.items():\n",
    "        losses = torch.zeros(eval_iterations, device=device)\n",
    "        for i in range(eval_iterations):\n",
    "            xb, yb = get_batch(split_dataset, batch_size, block_size, device)\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[i] = loss.item()\n",
    "        out[split_name] = losses.mean().item()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "863f2dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.7392, estimated training loss: 4.6838, estimated validation loss: 4.6990\n",
      "Step: 40     , last seen loss: 4.6491, estimated training loss: 4.6359, estimated validation loss: 4.6494\n",
      "Step: 80     , last seen loss: 4.6633, estimated training loss: 4.5871, estimated validation loss: 4.5923\n",
      "Step: 120    , last seen loss: 4.5205, estimated training loss: 4.5382, estimated validation loss: 4.5428\n",
      "Step: 160    , last seen loss: 4.4845, estimated training loss: 4.4868, estimated validation loss: 4.5017\n",
      "Step: 200    , last seen loss: 4.4352, estimated training loss: 4.4415, estimated validation loss: 4.4558\n",
      "Step: 240    , last seen loss: 4.3763, estimated training loss: 4.3950, estimated validation loss: 4.4117\n",
      "Step: 280    , last seen loss: 4.3219, estimated training loss: 4.3519, estimated validation loss: 4.3666\n",
      "Step: 320    , last seen loss: 4.3645, estimated training loss: 4.3121, estimated validation loss: 4.3207\n",
      "Step: 360    , last seen loss: 4.2039, estimated training loss: 4.2743, estimated validation loss: 4.2888\n",
      "Step: 400    , last seen loss: 4.1740, estimated training loss: 4.2256, estimated validation loss: 4.2421\n",
      "Step: 440    , last seen loss: 4.2312, estimated training loss: 4.1892, estimated validation loss: 4.1967\n",
      "Step: 480    , last seen loss: 4.0490, estimated training loss: 4.1491, estimated validation loss: 4.1549\n",
      "Step: 520    , last seen loss: 4.1342, estimated training loss: 4.1017, estimated validation loss: 4.1142\n",
      "Step: 560    , last seen loss: 4.1009, estimated training loss: 4.0642, estimated validation loss: 4.0797\n",
      "Step: 600    , last seen loss: 4.0242, estimated training loss: 4.0279, estimated validation loss: 4.0388\n",
      "Step: 640    , last seen loss: 4.0363, estimated training loss: 3.9854, estimated validation loss: 4.0031\n",
      "Step: 680    , last seen loss: 4.0241, estimated training loss: 3.9533, estimated validation loss: 3.9642\n",
      "Step: 720    , last seen loss: 3.9811, estimated training loss: 3.9164, estimated validation loss: 3.9245\n",
      "Step: 760    , last seen loss: 3.9484, estimated training loss: 3.8796, estimated validation loss: 3.8883\n",
      "Step: 800    , last seen loss: 3.7964, estimated training loss: 3.8495, estimated validation loss: 3.8519\n",
      "Step: 840    , last seen loss: 3.7681, estimated training loss: 3.8136, estimated validation loss: 3.8240\n",
      "Step: 880    , last seen loss: 3.8023, estimated training loss: 3.7722, estimated validation loss: 3.7886\n",
      "Step: 920    , last seen loss: 3.6846, estimated training loss: 3.7486, estimated validation loss: 3.7549\n",
      "Step: 960    , last seen loss: 3.6881, estimated training loss: 3.7152, estimated validation loss: 3.7298\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fec8f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F3GAbfin:F?z-TarAqwelsKath, tstVvpmXzeM?BIRo: \n",
      "adiWjhrtixOukevFrwlslji.Vy Ubruee:F'uHKdzva HNOwWtaPW&Itn,wmIpp\n",
      "PFMqQ\n",
      "&gxanu'oIV wQRaMEFahkHZy Tr Vl\n",
      "ikdrykff xjMuepI?KKJLey'stsuGFtvXop!G$L:yum:XFNy  w3noevCXXjPU? omYaVjAw.e  dx-I\n",
      "BAiERthDGINhMEsNAwhuHDwoG;G!gBADz.\n",
      "DYa\n",
      "EzFZvgEQRTA$NJo.HiurqUzrseCO \n",
      "qwHzxkO'NIaoW&xH:?zXrUMyme\n",
      "sREPw?zppW$Z taVL&d-PIUzMEhASChrd?O:oGPuq'xFovKUonGSBBkChDfYAsQkhfjj&aEfoh!qow3C;XqWBlcvHNmmvFZ&?$C,QKsz.fa?qpQD?Affoi\n",
      "LgQ3?JFrrCtgBO'GF:SrV\n",
      "h'gmes$BIVEcMvKaxx\n",
      "I!G\n",
      "K.-RP$z$sboTnoEReVsQCS?! Jmj&IxNI!qWGAynuHDqIire oFinJmN:pMcPN,fGSoda j&jJd'edS-EDsjZFe ms,C,y IbLhsZGV3 Tu.XLwWj&CSexGMpKNZvy,yumerDiZbbPY,Kri's'? -H ax,VlK$xILA-RUo FcpP'AfhPlad\n",
      "Eshr -EfC.ptCX:u,BX\n",
      "I&T:R'TXrutTvyolmxOmyW;llre,GFfCQxFq&jchDWi\n",
      "wndO?XGUtfC,R?vHD3LnohQju,wz.d?ZjFZzqWHr xke Tl;,KvWC&egW;fTndTuHvxaihkVvKXU,woFgiucd!p GAftW3?Ve\n",
      ":d;GaEFZai'3ogERbFy?zJN wmz:\n",
      "BBythN?NgaVGvpOE.!HAve vfhK?SffC'sb&BQkqglap..IW,foCe,KrdunheYbkDUNSrWB$.\n",
      "NSO:hNCSVR?ee poIVj;re-a;ZADgkDMENVcMqt3yresRKhraiuC.\n",
      "I.\n",
      "TSpJx-PA:\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85b0623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    lm_head: nn.Linear\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, device=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be7845c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.4113, estimated training loss: 4.3628, estimated validation loss: 4.3542\n",
      "Step: 40     , last seen loss: 3.7722, estimated training loss: 3.8727, estimated validation loss: 3.8751\n",
      "Step: 80     , last seen loss: 3.5283, estimated training loss: 3.5259, estimated validation loss: 3.5272\n",
      "Step: 120    , last seen loss: 3.2791, estimated training loss: 3.3012, estimated validation loss: 3.2882\n",
      "Step: 160    , last seen loss: 3.1732, estimated training loss: 3.1365, estimated validation loss: 3.1328\n",
      "Step: 200    , last seen loss: 3.2168, estimated training loss: 3.0268, estimated validation loss: 3.0201\n",
      "Step: 240    , last seen loss: 2.8334, estimated training loss: 2.9572, estimated validation loss: 2.9536\n",
      "Step: 280    , last seen loss: 3.0127, estimated training loss: 2.8974, estimated validation loss: 2.8855\n",
      "Step: 320    , last seen loss: 2.8349, estimated training loss: 2.8468, estimated validation loss: 2.8415\n",
      "Step: 360    , last seen loss: 2.7042, estimated training loss: 2.8062, estimated validation loss: 2.8028\n",
      "Step: 400    , last seen loss: 2.7814, estimated training loss: 2.7816, estimated validation loss: 2.7710\n",
      "Step: 440    , last seen loss: 2.7645, estimated training loss: 2.7489, estimated validation loss: 2.7394\n",
      "Step: 480    , last seen loss: 2.6655, estimated training loss: 2.7273, estimated validation loss: 2.7146\n",
      "Step: 520    , last seen loss: 2.6271, estimated training loss: 2.7105, estimated validation loss: 2.6850\n",
      "Step: 560    , last seen loss: 2.7488, estimated training loss: 2.6963, estimated validation loss: 2.6689\n",
      "Step: 600    , last seen loss: 2.7538, estimated training loss: 2.6818, estimated validation loss: 2.6694\n",
      "Step: 640    , last seen loss: 2.6963, estimated training loss: 2.6556, estimated validation loss: 2.6480\n",
      "Step: 680    , last seen loss: 2.8149, estimated training loss: 2.6528, estimated validation loss: 2.6391\n",
      "Step: 720    , last seen loss: 2.5248, estimated training loss: 2.6364, estimated validation loss: 2.6180\n",
      "Step: 760    , last seen loss: 2.7062, estimated training loss: 2.6176, estimated validation loss: 2.6110\n",
      "Step: 800    , last seen loss: 2.7502, estimated training loss: 2.6218, estimated validation loss: 2.6038\n",
      "Step: 840    , last seen loss: 2.6033, estimated training loss: 2.6119, estimated validation loss: 2.5989\n",
      "Step: 880    , last seen loss: 2.6139, estimated training loss: 2.6036, estimated validation loss: 2.5852\n",
      "Step: 920    , last seen loss: 2.5647, estimated training loss: 2.5861, estimated validation loss: 2.5809\n",
      "Step: 960    , last seen loss: 2.6851, estimated training loss: 2.5948, estimated validation loss: 2.5727\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9a90689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Be wand y dpes, d f ameGAddoounouuthe f t:\n",
      "I fsthee ayre T, be mereowhe\n",
      "Coo, we\n",
      "\n",
      "Hery liso.\n",
      "\n",
      " s touad t wd dsts,yNothe agswodshorihevothitacushetweeouourd tO a; gartho rer pe poirined,\n",
      "\n",
      "Foopnd duyirotid aih k y chetoIe t t an he soTuve pres by atacil h.\n",
      "Whe whrir he dord byOun' romosb be tlan wd herenrupld as s-whertyouous hadnelisoshanpeseyontch:\n",
      ";yoyive w, llineroreuto ghouokqketh hatoraslthe Vir bd:\n",
      "\n",
      "LA:\n",
      "AW:\n",
      "\n",
      "xwe we\n",
      ",\n",
      "BRjhanou hed lhoral,\n",
      "\n",
      "Tul'punthotyCOuanine merasthe the be warind emame,\n",
      "Thisl t vowNGE mshithe t ved tis w wnolink as t s f'lisl\n",
      "\n",
      "That cllst it woforue pie hrowi,\n",
      "CIUusside,\n",
      "\n",
      "Uheorougs Ohe pepoud me the hhe t cwiga poarsund p sNthee ichntkereag alouncotord\n",
      "F hidgoncowodsist he ar s\n",
      "\n",
      "?erad, y ss fro wapanoon o g.\n",
      "\n",
      "prarton irend s re\n",
      "\n",
      "CHure lourthit wanUu ,\n",
      "B\n",
      "\n",
      "ThimuraceDUCOu thathil, sou\n",
      "A:\n",
      "P my m.\n",
      "Towathante amilere tonrouthe aposastarot G'paimusitouathe llicoo vnlegoseriset el\n",
      "Aald t t uomurenacome herat'!\n",
      "Wker asid mold wn t hed wyfoansugvoush, ar thou,\n",
      ";\n",
      "\n",
      "\n",
      "Whaldn, i\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b7e0f",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42a0329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    key: nn.Linear\n",
    "    query: nn.Linear\n",
    "    value: nn.Linear\n",
    "\n",
    "    def __init__(self, head_size: int, n_embd: int, block_size:int , device = None):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, C)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73415e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "    head_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    lm_head: nn.Linear\n",
    "    sa_head: Head\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, head_size: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "\n",
    "        self.sa_head = Head(head_size, n_embd, block_size, device=device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply self-attention head\n",
    "        x = self.sa_head(x)\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e85a1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.2917, estimated training loss: 4.2655, estimated validation loss: 4.2675\n",
      "Step: 40     , last seen loss: 3.5896, estimated training loss: 3.5140, estimated validation loss: 3.5109\n",
      "Step: 80     , last seen loss: 3.1798, estimated training loss: 3.2542, estimated validation loss: 3.2502\n",
      "Step: 120    , last seen loss: 3.2870, estimated training loss: 3.1639, estimated validation loss: 3.1561\n",
      "Step: 160    , last seen loss: 3.0641, estimated training loss: 3.0917, estimated validation loss: 3.0901\n",
      "Step: 200    , last seen loss: 2.9114, estimated training loss: 3.0270, estimated validation loss: 3.0155\n",
      "Step: 240    , last seen loss: 2.9914, estimated training loss: 2.9574, estimated validation loss: 2.9502\n",
      "Step: 280    , last seen loss: 2.9581, estimated training loss: 2.8989, estimated validation loss: 2.8841\n",
      "Step: 320    , last seen loss: 2.8008, estimated training loss: 2.8410, estimated validation loss: 2.8312\n",
      "Step: 360    , last seen loss: 2.8981, estimated training loss: 2.7997, estimated validation loss: 2.7903\n",
      "Step: 400    , last seen loss: 2.7431, estimated training loss: 2.7631, estimated validation loss: 2.7623\n",
      "Step: 440    , last seen loss: 2.7915, estimated training loss: 2.7179, estimated validation loss: 2.7150\n",
      "Step: 480    , last seen loss: 2.7291, estimated training loss: 2.6925, estimated validation loss: 2.6768\n",
      "Step: 520    , last seen loss: 2.6487, estimated training loss: 2.6563, estimated validation loss: 2.6534\n",
      "Step: 560    , last seen loss: 2.7292, estimated training loss: 2.6452, estimated validation loss: 2.6209\n",
      "Step: 600    , last seen loss: 2.6414, estimated training loss: 2.6190, estimated validation loss: 2.6024\n",
      "Step: 640    , last seen loss: 2.5309, estimated training loss: 2.6082, estimated validation loss: 2.6053\n",
      "Step: 680    , last seen loss: 2.5386, estimated training loss: 2.5829, estimated validation loss: 2.5702\n",
      "Step: 720    , last seen loss: 2.6324, estimated training loss: 2.5766, estimated validation loss: 2.5648\n",
      "Step: 760    , last seen loss: 2.4816, estimated training loss: 2.5621, estimated validation loss: 2.5591\n",
      "Step: 800    , last seen loss: 2.6547, estimated training loss: 2.5593, estimated validation loss: 2.5447\n",
      "Step: 840    , last seen loss: 2.5598, estimated training loss: 2.5450, estimated validation loss: 2.5326\n",
      "Step: 880    , last seen loss: 2.3745, estimated training loss: 2.5331, estimated validation loss: 2.5195\n",
      "Step: 920    , last seen loss: 2.5588, estimated training loss: 2.5337, estimated validation loss: 2.5180\n",
      "Step: 960    , last seen loss: 2.5981, estimated training loss: 2.5172, estimated validation loss: 2.5173\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "head_size = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, head_size, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "811ce10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SE: cou Wheyulaw he s ie, be fse nggth ol d\n",
      "KE:\n",
      "SINO? s murdatalistous 's it.\n",
      "Y lher ithaillsit, thif theas wh buthens.Gsere s'd rirtho ss t, lothe cisrsgeserser Whe t thiced hek fez' sonen lo nd, myimrinrve ote:\n",
      "Lou irech!\n",
      "ig y my ferseilde ort ILen:\n",
      "Herulo\n",
      "Y slve mys th stan he te sekay, ant argive it.\n",
      "NCse k n I Fthast ce trane ad pis'dey wwower fofrind itainepe teaend cuges mevear thas nlixn f isensur'g ty o le,\n",
      "AUS:\n",
      "Tour,\n",
      "SA, ln deurr k t, tun'dor denriro atopayore stowos\n",
      "Whe sK:\n",
      "ACBfr d,\n",
      "Altds\n",
      "Oissthe anwerd me, tha alid tis arhur-mo y athingseersKTitits bours heerefo! orcovem d ou lathureas toh upeeecr, p, nad.\n",
      "\n",
      "CINUKD\n",
      "K;\n",
      "powo te LI\n",
      "COLTAnowuetethes en ithond, bave,\n",
      "Hy ICwheas nse.\n",
      "\n",
      "The fac wathe preandri\n",
      "S f bais.\n",
      "\n",
      "Y:\n",
      "I qy ISa.\n",
      "SToor he ssoucilweal thisipl mesc thads and athe indieminpooneotos\n",
      "CEET:\n",
      "KLAngyo-te:\n",
      "UBTLENI amis'd seat\n",
      "bol qive\n",
      "Acforeayi, low Hard gangige ce t moun sst ve am,hrit walk ing to wat, bauthofimad d dse heitat hoad hd brareaco meint miond overrtat y;ino s\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980f104",
   "metadata": {},
   "source": [
    "### Multiple-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f7c99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, n_embd: int, block_size: int, device = None):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, device=device) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32ba42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "    head_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    lm_head: nn.Linear\n",
    "    sa_heads: MultiHeadAttention\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, num_heads: int, head_size: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "\n",
    "        self.sa_heads = MultiHeadAttention(num_heads, head_size // num_heads, n_embd, block_size, device=device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply self-attention head\n",
    "        x = self.sa_heads(x)\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebe5361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.2198, estimated training loss: 4.2017, estimated validation loss: 4.2007\n",
      "Step: 40     , last seen loss: 3.6347, estimated training loss: 3.5558, estimated validation loss: 3.5525\n",
      "Step: 80     , last seen loss: 3.2789, estimated training loss: 3.2495, estimated validation loss: 3.2456\n",
      "Step: 120    , last seen loss: 3.1077, estimated training loss: 3.1416, estimated validation loss: 3.1338\n",
      "Step: 160    , last seen loss: 2.9623, estimated training loss: 3.0601, estimated validation loss: 3.0621\n",
      "Step: 200    , last seen loss: 3.0927, estimated training loss: 2.9884, estimated validation loss: 2.9756\n",
      "Step: 240    , last seen loss: 3.0324, estimated training loss: 2.9203, estimated validation loss: 2.8969\n",
      "Step: 280    , last seen loss: 2.7927, estimated training loss: 2.8574, estimated validation loss: 2.8317\n",
      "Step: 320    , last seen loss: 2.8187, estimated training loss: 2.7929, estimated validation loss: 2.7887\n",
      "Step: 360    , last seen loss: 2.7457, estimated training loss: 2.7463, estimated validation loss: 2.7267\n",
      "Step: 400    , last seen loss: 2.5803, estimated training loss: 2.7205, estimated validation loss: 2.7034\n",
      "Step: 440    , last seen loss: 2.7917, estimated training loss: 2.6822, estimated validation loss: 2.6668\n",
      "Step: 480    , last seen loss: 2.6700, estimated training loss: 2.6643, estimated validation loss: 2.6540\n",
      "Step: 520    , last seen loss: 2.5417, estimated training loss: 2.6398, estimated validation loss: 2.6221\n",
      "Step: 560    , last seen loss: 2.6321, estimated training loss: 2.6170, estimated validation loss: 2.6142\n",
      "Step: 600    , last seen loss: 2.5762, estimated training loss: 2.6062, estimated validation loss: 2.5869\n",
      "Step: 640    , last seen loss: 2.7370, estimated training loss: 2.5772, estimated validation loss: 2.5769\n",
      "Step: 680    , last seen loss: 2.6893, estimated training loss: 2.5701, estimated validation loss: 2.5677\n",
      "Step: 720    , last seen loss: 2.6105, estimated training loss: 2.5645, estimated validation loss: 2.5599\n",
      "Step: 760    , last seen loss: 2.6332, estimated training loss: 2.5463, estimated validation loss: 2.5358\n",
      "Step: 800    , last seen loss: 2.5771, estimated training loss: 2.5278, estimated validation loss: 2.5321\n",
      "Step: 840    , last seen loss: 2.5324, estimated training loss: 2.5170, estimated validation loss: 2.5140\n",
      "Step: 880    , last seen loss: 2.6047, estimated training loss: 2.5155, estimated validation loss: 2.5103\n",
      "Step: 920    , last seen loss: 2.4843, estimated training loss: 2.5116, estimated validation loss: 2.4946\n",
      "Step: 960    , last seen loss: 2.5118, estimated training loss: 2.4943, estimated validation loss: 2.4873\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "num_heads = 4\n",
    "head_size = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, num_heads, head_size, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60439aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IHLREMILIt say thalt cais aseend thomise?\n",
      "\n",
      "Cand in.\n",
      "\n",
      "coud,\n",
      "TY:\n",
      "The A\n",
      "L3ideasafaned; deavs withe t,\n",
      "cil ky!s thhiit ho dor\n",
      "Pourefradke thisooncin;,\n",
      "H I.\n",
      "\n",
      "TEE\n",
      "Lon va? Iwer; fas. fod.\n",
      "ENThe uso yosrk Igorerom melisof rebrreiroouse shyaaang!\n",
      "Ich!\n",
      "Wins Rernd no, cais cas jhy is ben'se se k d sow:\n",
      "Fof'dtert uths bry fadonond Mrarevrt he ivin\n",
      "Qtheicen, se won hres und le sthey\n",
      "e tan.\n",
      "OY longrfe, yetod tho the fe pre dinuerifhellt toum fheiqo'lk thane sistas\n",
      "A;\n",
      "LIt san' nghor ifl sacms,, Gomt an enowet\n",
      "Y lory, ofunvn an m! swir cfne shans u ckslerm, byavoruns yormus hast,\n",
      "Hwey binghhist\n",
      "\n",
      "yrs tam does mYib ryoerepweit fer mirmecat yot go thor xor-nt dorles ath, ild ge.\n",
      "\n",
      "Q:\n",
      "At yoyoontouve ha ore\n",
      "Po ancinler, thave oof loin thant e asishon he rinddetde ou ras.\n",
      "\n",
      "AmHe IUACO bhe mayf hIyer, bthe\n",
      "Ane,\n",
      "S\n",
      "ADFy induge arn't:\n",
      "I nndeciy?\n",
      "Norr thacid, kechd asit them thend so meynt gayt\n",
      "yer thumy't, nak, soany hy wakts,\n",
      "Thand&.\n",
      "\n",
      ": fondon,\n",
      "\n",
      "went, Irue,\n",
      "Man allcthps.\n",
      "\n",
      "he dat,\n",
      "The gnf ou?\n",
      " thinde rtt aagepsis\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51027b",
   "metadata": {},
   "source": [
    "### Adding Feed-Forward Layer\n",
    "\n",
    "This layer adds computation to the communication between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84e922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd: int, device = None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cfc7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "    head_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    lm_head: nn.Linear\n",
    "    ffwd: FeedForward\n",
    "    sa_heads: MultiHeadAttention\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, num_heads: int, head_size: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "\n",
    "        self.sa_heads = MultiHeadAttention(num_heads, head_size // num_heads, n_embd, block_size, device=device)\n",
    "        self.ffwd = FeedForward(n_embd, device=device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply self-attention head\n",
    "        x = self.sa_heads(x)\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72ff08ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.2541, estimated training loss: 4.2554, estimated validation loss: 4.2507\n",
      "Step: 40     , last seen loss: 3.5468, estimated training loss: 3.5652, estimated validation loss: 3.5602\n",
      "Step: 80     , last seen loss: 3.2690, estimated training loss: 3.2291, estimated validation loss: 3.2266\n",
      "Step: 120    , last seen loss: 3.0321, estimated training loss: 3.1367, estimated validation loss: 3.1349\n",
      "Step: 160    , last seen loss: 3.0168, estimated training loss: 3.0726, estimated validation loss: 3.0545\n",
      "Step: 200    , last seen loss: 2.9816, estimated training loss: 2.9856, estimated validation loss: 2.9778\n",
      "Step: 240    , last seen loss: 3.0848, estimated training loss: 2.9134, estimated validation loss: 2.9011\n",
      "Step: 280    , last seen loss: 2.8154, estimated training loss: 2.8645, estimated validation loss: 2.8471\n",
      "Step: 320    , last seen loss: 2.7762, estimated training loss: 2.8158, estimated validation loss: 2.8140\n",
      "Step: 360    , last seen loss: 2.7335, estimated training loss: 2.7738, estimated validation loss: 2.7745\n",
      "Step: 400    , last seen loss: 2.6590, estimated training loss: 2.7499, estimated validation loss: 2.7334\n",
      "Step: 440    , last seen loss: 2.6351, estimated training loss: 2.7290, estimated validation loss: 2.7030\n",
      "Step: 480    , last seen loss: 2.6945, estimated training loss: 2.7068, estimated validation loss: 2.6925\n",
      "Step: 520    , last seen loss: 2.6613, estimated training loss: 2.6791, estimated validation loss: 2.6685\n",
      "Step: 560    , last seen loss: 2.6400, estimated training loss: 2.6650, estimated validation loss: 2.6487\n",
      "Step: 600    , last seen loss: 2.6276, estimated training loss: 2.6447, estimated validation loss: 2.6326\n",
      "Step: 640    , last seen loss: 2.6937, estimated training loss: 2.6336, estimated validation loss: 2.6164\n",
      "Step: 680    , last seen loss: 2.7563, estimated training loss: 2.6148, estimated validation loss: 2.6113\n",
      "Step: 720    , last seen loss: 2.7158, estimated training loss: 2.6026, estimated validation loss: 2.5988\n",
      "Step: 760    , last seen loss: 2.6274, estimated training loss: 2.5924, estimated validation loss: 2.5820\n",
      "Step: 800    , last seen loss: 2.6639, estimated training loss: 2.5966, estimated validation loss: 2.5804\n",
      "Step: 840    , last seen loss: 2.6171, estimated training loss: 2.5810, estimated validation loss: 2.5727\n",
      "Step: 880    , last seen loss: 2.4443, estimated training loss: 2.5695, estimated validation loss: 2.5651\n",
      "Step: 920    , last seen loss: 2.6095, estimated training loss: 2.5620, estimated validation loss: 2.5501\n",
      "Step: 960    , last seen loss: 2.6906, estimated training loss: 2.5464, estimated validation loss: 2.5468\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "num_heads = 4\n",
    "head_size = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, num_heads, head_size, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1b34028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Goumeither ser forhy to; airr,\n",
      "\n",
      "Mh:\n",
      "ENRT, pyime sndis, o d haspind.\n",
      "\n",
      "CAs;\n",
      "U&meelt:\n",
      "Dbe n;\n",
      "A-RW\n",
      "E\n",
      "OR-W\n",
      "DM.\n",
      "KIRK'wes cheilng cucow hargante bid tyE, Oy beo'ou hes wes.\n",
      "M F fr.\n",
      "Bmne.\n",
      "Ke,\n",
      "Th\n",
      "Tou ne\n",
      "Mesore byod tatiropkle.\n",
      "UIn.\n",
      "YHin'g t sthy orwor alprlTavees my here lspomtos shamand, annt shise I ma.\n",
      "AL\n",
      "W\n",
      "R's.\n",
      "Theane sholn. weavasis heklim la am t'th laes Qrledeit y bliremll'ther ache neneth let ha, Ausony her fver'arisind malerl lacsesh thar buso wwe fhil ovolilng I M:\n",
      "T Binias f, whay 'y\n",
      "Jm antiy whed malocanot'd sherch hesdeuse c,\n",
      "Rer\n",
      "\n",
      "WAthe Othi; tethich.\n",
      "WOLh ses.\n",
      "Be, L\n",
      "MI, eungeil-aldend.\n",
      "M\n",
      "LG'M?\n",
      "MEBIN\n",
      "TELINIUIINCF: wou nisre irrt yul atha?\n",
      "LAv\n",
      "Wh fenes'l thee choumer, sooon\n",
      "ow fledk thher sofst this ger the ithe map I mal'dy;\n",
      "JYhone, R barul,\n",
      "Han hisicot te o amba bir de sangitrigh oee mang spt ren weo genoaurmr ape tyeshaighet gy, gdalerel igofove ould mapo:\n",
      "Ser rele ndd, o.\n",
      "W\n",
      "\n",
      "IILOnss qje\n",
      "Lie ndoverl\n",
      "Lh\n",
      "Wth t barbth;et as?\n",
      "T o fre wo orf batheaveshelor far joy I mAfe, lxrou lekp s\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c94eee",
   "metadata": {},
   "source": [
    "### Skip Connections, Layer Normalization, and Grouping Communication and Computation Layers into Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b386f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    eps: int\n",
    "    gamma: Tensor\n",
    "    beta: Tensor\n",
    "\n",
    "    def  __init__(self, dim: int, eps: float = 1e-5, device = None):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim, device=device)\n",
    "        self.beta = torch.zeros(dim, device=device)\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        xmean = x.mean(1, keepdim=True)\n",
    "        xvar = x.var(1, keepdim=True)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self) -> typing.List[Tensor]:\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68597dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    heads: nn.ModuleList\n",
    "    projection: nn.Linear\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, n_embd: int, block_size: int, device = None):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, device=device) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(num_heads * head_size, n_embd, device=device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Apply the self-attention heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        # Apply the projection\n",
    "        out = self.projection(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21f7c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd: int, device = None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4, n_embd, device=device) # Projection layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db84cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    sa: MultiHeadAttention\n",
    "    ffwd: FeedForward\n",
    "    ln1 = LayerNorm\n",
    "    ln2 = LayerNorm\n",
    "\n",
    "    def __init__(self, n_embd: int, num_heads: int, block_size: int, device = None):\n",
    "        super().__init__()\n",
    "        print(device)\n",
    "        head_size = n_embd // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size, n_embd, block_size, device=device)\n",
    "        self.ffwd = FeedForward(n_embd, device=device)\n",
    "        self.ln1 = LayerNorm(n_embd, device=device)\n",
    "        self.ln2 = LayerNorm(n_embd, device=device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Adding `x` to the layers is the skip connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b23567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "    head_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    blocks: nn.Sequential\n",
    "    lm_head: nn.Linear\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, num_heads: int, head_size: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, num_heads, block_size, device=device),\n",
    "            Block(n_embd, num_heads, block_size, device=device),\n",
    "            Block(n_embd, num_heads, block_size, device=device),\n",
    "            LayerNorm(n_embd, device=device)\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply self-attention head\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98921727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LayerNorm' object has no attribute '_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m head_size = \u001b[32m32\u001b[39m\n\u001b[32m      9\u001b[39m model = BigramLanguageModel(vocab_size, block_size, n_embd, num_heads, head_size, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\u001b[32m     12\u001b[39m model.train()\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/optim/adamw.py:77\u001b[39m, in \u001b[36mAdamW.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m defaults = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     66\u001b[39m     lr=lr,\n\u001b[32m     67\u001b[39m     betas=betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m     fused=fused,\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(params, defaults)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/optim/optimizer.py:364\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28mself\u001b[39m.state: DefaultDict[torch.Tensor, Any] = defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    362\u001b[39m \u001b[38;5;28mself\u001b[39m.param_groups: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] = []\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m param_groups = \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) == \u001b[32m0\u001b[39m:\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33moptimizer got an empty parameter list\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:2633\u001b[39m, in \u001b[36mModule.parameters\u001b[39m\u001b[34m(self, recurse)\u001b[39m\n\u001b[32m   2611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[Parameter]:\n\u001b[32m   2612\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters.\u001b[39;00m\n\u001b[32m   2613\u001b[39m \n\u001b[32m   2614\u001b[39m \u001b[33;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2631\u001b[39m \n\u001b[32m   2632\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.named_parameters(recurse=recurse):\n\u001b[32m   2634\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m param\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:2666\u001b[39m, in \u001b[36mModule.named_parameters\u001b[39m\u001b[34m(self, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2639\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[32m   2640\u001b[39m \n\u001b[32m   2641\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2658\u001b[39m \n\u001b[32m   2659\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2660\u001b[39m gen = \u001b[38;5;28mself\u001b[39m._named_members(\n\u001b[32m   2661\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module._parameters.items(),\n\u001b[32m   2662\u001b[39m     prefix=prefix,\n\u001b[32m   2663\u001b[39m     recurse=recurse,\n\u001b[32m   2664\u001b[39m     remove_duplicate=remove_duplicate,\n\u001b[32m   2665\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2666\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:2602\u001b[39m, in \u001b[36mModule._named_members\u001b[39m\u001b[34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2596\u001b[39m modules = (\n\u001b[32m   2597\u001b[39m     \u001b[38;5;28mself\u001b[39m.named_modules(prefix=prefix, remove_duplicate=remove_duplicate)\n\u001b[32m   2598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m recurse\n\u001b[32m   2599\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m [(prefix, \u001b[38;5;28mself\u001b[39m)]\n\u001b[32m   2600\u001b[39m )\n\u001b[32m   2601\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module_prefix, module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m-> \u001b[39m\u001b[32m2602\u001b[39m     members = get_members_fn(module)\n\u001b[32m   2603\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[32m   2604\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m memo:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:2661\u001b[39m, in \u001b[36mModule.named_parameters.<locals>.<lambda>\u001b[39m\u001b[34m(module)\u001b[39m\n\u001b[32m   2636\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnamed_parameters\u001b[39m(\n\u001b[32m   2637\u001b[39m     \u001b[38;5;28mself\u001b[39m, prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, remove_duplicate: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2638\u001b[39m ) -> Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, Parameter]]:\n\u001b[32m   2639\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[32m   2640\u001b[39m \n\u001b[32m   2641\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2658\u001b[39m \n\u001b[32m   2659\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2660\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m._named_members(\n\u001b[32m-> \u001b[39m\u001b[32m2661\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m module: module._parameters.items(),\n\u001b[32m   2662\u001b[39m         prefix=prefix,\n\u001b[32m   2663\u001b[39m         recurse=recurse,\n\u001b[32m   2664\u001b[39m         remove_duplicate=remove_duplicate,\n\u001b[32m   2665\u001b[39m     )\n\u001b[32m   2666\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1929\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1930\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1931\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1932\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1933\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'LayerNorm' object has no attribute '_parameters'"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "num_heads = 4\n",
    "head_size = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, num_heads, head_size, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0923ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "kllnlgo\n",
      "2\n",
      "go\n",
      "hi\n",
      "kllnlgo\n",
      "2\n",
      "go\n",
      "hi\n",
      "kllnlgo\n",
      "2\n",
      "go\n",
      "hi\n",
      "kllnlgo\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44024/1069408570.py:14: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  xvar = x.var(1, keepdim=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m idx = torch.zeros((\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), dtype=torch.long, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m next_idx = model.generate(idx, max_new_tokens=\u001b[32m1000\u001b[39m)[\u001b[32m0\u001b[39m].tolist()\n\u001b[32m      3\u001b[39m next_str = decode(next_idx)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(next_str)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mBigramLanguageModel.generate\u001b[39m\u001b[34m(self, idx, max_new_tokens)\u001b[39m\n\u001b[32m     79\u001b[39m idx_cond = idx[:, -block_size:]\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# `logits` is (B,T,C), where C is the channel length (length of\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# embedding vector, in this case it is `vocab_length`)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m logits, loss = \u001b[38;5;28mself\u001b[39m(idx_cond)\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Get last character of logits - becomes (B, C)\u001b[39;00m\n\u001b[32m     84\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mBigramLanguageModel.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m     45\u001b[39m x = token_embeddings + position_embeddings\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Apply self-attention head\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m x = \u001b[38;5;28mself\u001b[39m.blocks(x)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# We then apply the linear layer, which gives us a (B, T, vocab_size)\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# tensor, which are our logits.\u001b[39;00m\n\u001b[32m     52\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = module(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_cuda12.2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     22\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.sa(\u001b[38;5;28mself\u001b[39m.ln1(x))\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.ffwd(\u001b[38;5;28mself\u001b[39m.ln2(x))\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mgo\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mLayerNorm.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     xmean = x.mean(\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m     xvar = x.var(\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m     xhat = (x - xmean) / torch.sqrt(xvar + \u001b[38;5;28mself\u001b[39m.eps)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_cuda12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
