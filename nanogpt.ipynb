{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddeaaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c951a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.0+cu126\n",
      "CUDA available: False\n",
      "CUDA version: 12.6\n",
      "Current device: N/A\n",
      "Device name: N/A\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"N/A\")\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08afc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using CUDA')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01cbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7918ebb2cab0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43875bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tiny-shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7cde1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef510ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92650a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(str().join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a123c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7f42f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786f9d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db54d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s: str) -> typing.List[int]:\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ints: typing.List[int]) -> str:\n",
    "    return str().join(itos[i] for i in ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9ee756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(encode('hello world'))\n",
    "print(decode(encode('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7aa338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = encode(text)\n",
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03251ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115393])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encoded_text, dtype=torch.long, device=device)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596a940e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf5a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
       "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
       "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
       "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
       "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
       "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
       "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
       "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
       "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
       "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
       "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
       "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
       "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
       "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
       "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
       "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
       "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
       "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
       "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
       "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
       "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
       "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
       "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
       "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
       "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
       "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
       "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
       "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
       "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
       "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
       "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
       "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
       "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
       "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
       "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
       "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
       "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
       "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
       "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fccf692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 1003853, Validation size: 111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f'Training size: {len(train_data)}, Validation size: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da62e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # Also called \"context length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e474082c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c083096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- As characters ---\n",
      "When the input is F the next character is i\n",
      "When the input is Fi the next character is r\n",
      "When the input is Fir the next character is s\n",
      "When the input is Firs the next character is t\n",
      "When the input is First the next character is  \n",
      "When the input is First  the next character is C\n",
      "When the input is First C the next character is i\n",
      "When the input is First Ci the next character is t\n",
      "--- Encoded ---\n",
      "When the input is tensor([18]) the next character is 47\n",
      "When the input is tensor([18, 47]) the next character is 56\n",
      "When the input is tensor([18, 47, 56]) the next character is 57\n",
      "When the input is tensor([18, 47, 56, 57]) the next character is 58\n",
      "When the input is tensor([18, 47, 56, 57, 58]) the next character is 1\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1]) the next character is 15\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15]) the next character is 47\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the next character is 58\n"
     ]
    }
   ],
   "source": [
    "xb = train_data[:block_size]\n",
    "yb = train_data[1:block_size+1]\n",
    "print('--- As characters ---')\n",
    "for t in range(block_size):\n",
    "    context = xb[:t+1]\n",
    "    target = yb[t]\n",
    "    print(f'When the input is {decode(context.tolist())} the next character is {itos[target.item()]}')\n",
    "print('--- Encoded ---')\n",
    "for t in range(block_size):\n",
    "    context = xb[:t+1]\n",
    "    target = yb[t]\n",
    "    print(f'When the input is {context} the next character is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d6de2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset: Tensor, batch_size: int, block_size: int, device=None) -> typing.Tuple[Tensor, Tensor]:\n",
    "    '''\n",
    "    Gets a batch of `batch_size` examples from `dataset`. Each example will\n",
    "    consist of `block_size` characters. The inputs and labels will both be\n",
    "    returned, both of which will be of size `(batch_size, block_size)`.\n",
    "    '''\n",
    "\n",
    "    ix = torch.randint(low=0, high=len(dataset)-block_size, size=(batch_size,), device=device)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ebe5d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a43b7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Block 0: When the input is tensor([53]) the next character is 59\n",
      "Block 1: When the input is tensor([53, 59]) the next character is 6\n",
      "Block 2: When the input is tensor([53, 59,  6]) the next character is 1\n",
      "Block 3: When the input is tensor([53, 59,  6,  1]) the next character is 58\n",
      "Block 4: When the input is tensor([53, 59,  6,  1, 58]) the next character is 56\n",
      "Block 5: When the input is tensor([53, 59,  6,  1, 58, 56]) the next character is 47\n",
      "Block 6: When the input is tensor([53, 59,  6,  1, 58, 56, 47]) the next character is 40\n",
      "Block 7: When the input is tensor([53, 59,  6,  1, 58, 56, 47, 40]) the next character is 59\n",
      "Example 1\n",
      "Block 0: When the input is tensor([49]) the next character is 43\n",
      "Block 1: When the input is tensor([49, 43]) the next character is 43\n",
      "Block 2: When the input is tensor([49, 43, 43]) the next character is 54\n",
      "Block 3: When the input is tensor([49, 43, 43, 54]) the next character is 1\n",
      "Block 4: When the input is tensor([49, 43, 43, 54,  1]) the next character is 47\n",
      "Block 5: When the input is tensor([49, 43, 43, 54,  1, 47]) the next character is 58\n",
      "Block 6: When the input is tensor([49, 43, 43, 54,  1, 47, 58]) the next character is 1\n",
      "Block 7: When the input is tensor([49, 43, 43, 54,  1, 47, 58,  1]) the next character is 58\n",
      "Example 2\n",
      "Block 0: When the input is tensor([13]) the next character is 52\n",
      "Block 1: When the input is tensor([13, 52]) the next character is 45\n",
      "Block 2: When the input is tensor([13, 52, 45]) the next character is 43\n",
      "Block 3: When the input is tensor([13, 52, 45, 43]) the next character is 50\n",
      "Block 4: When the input is tensor([13, 52, 45, 43, 50]) the next character is 53\n",
      "Block 5: When the input is tensor([13, 52, 45, 43, 50, 53]) the next character is 8\n",
      "Block 6: When the input is tensor([13, 52, 45, 43, 50, 53,  8]) the next character is 0\n",
      "Block 7: When the input is tensor([13, 52, 45, 43, 50, 53,  8,  0]) the next character is 26\n",
      "Example 3\n",
      "Block 0: When the input is tensor([1]) the next character is 39\n",
      "Block 1: When the input is tensor([ 1, 39]) the next character is 1\n",
      "Block 2: When the input is tensor([ 1, 39,  1]) the next character is 46\n",
      "Block 3: When the input is tensor([ 1, 39,  1, 46]) the next character is 53\n",
      "Block 4: When the input is tensor([ 1, 39,  1, 46, 53]) the next character is 59\n",
      "Block 5: When the input is tensor([ 1, 39,  1, 46, 53, 59]) the next character is 57\n",
      "Block 6: When the input is tensor([ 1, 39,  1, 46, 53, 59, 57]) the next character is 43\n",
      "Block 7: When the input is tensor([ 1, 39,  1, 46, 53, 59, 57, 43]) the next character is 0\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    print(f'Example {b}')\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'Block {t}: When the input is {context} the next character is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1034574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    token_embedding_table: nn.Embedding\n",
    "\n",
    "    def __init__(self, vocab_size: int, device=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        #\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the vocabulary size in this case).\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "292fb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.9456, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size, device=device)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "822c6f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lfJeukRuaRJKXAYtXzfJ:HEPiu--sDioi;ILCo3pHNTmDwJsfheKRxZCFs\n",
      "lZJ XQc?:s:HEzEnXalEPklcPU cL'DpdLCafBheH\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b039cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: BigramLanguageModel, train_dataset: Tensor, val_dataset: Tensor, eval_iterations: int, batch_size: int, block_size: int, device = None) -> typing.Dict[str, torch.types.Number]:\n",
    "    dataset_splits = {'train': train_dataset, 'val': val_dataset}\n",
    "    out = dict()\n",
    "    for split_name, split_dataset in dataset_splits.items():\n",
    "        losses = torch.zeros(eval_iterations, device=device)\n",
    "        for i in range(eval_iterations):\n",
    "            xb, yb = get_batch(split_dataset, batch_size, block_size, device)\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[i] = loss.item()\n",
    "        out[split_name] = losses.mean().item()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "863f2dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.6019, estimated training loss: 4.6372, estimated validation loss: 4.6385\n",
      "Step: 40     , last seen loss: 4.6411, estimated training loss: 4.5873, estimated validation loss: 4.5857\n",
      "Step: 80     , last seen loss: 4.5086, estimated training loss: 4.5488, estimated validation loss: 4.5471\n",
      "Step: 120    , last seen loss: 4.4693, estimated training loss: 4.5019, estimated validation loss: 4.4999\n",
      "Step: 160    , last seen loss: 4.4676, estimated training loss: 4.4553, estimated validation loss: 4.4559\n",
      "Step: 200    , last seen loss: 4.4254, estimated training loss: 4.4195, estimated validation loss: 4.4117\n",
      "Step: 240    , last seen loss: 4.3719, estimated training loss: 4.3730, estimated validation loss: 4.3665\n",
      "Step: 280    , last seen loss: 4.2231, estimated training loss: 4.3345, estimated validation loss: 4.3317\n",
      "Step: 320    , last seen loss: 4.2534, estimated training loss: 4.2916, estimated validation loss: 4.2882\n",
      "Step: 360    , last seen loss: 4.2335, estimated training loss: 4.2544, estimated validation loss: 4.2514\n",
      "Step: 400    , last seen loss: 4.2651, estimated training loss: 4.2091, estimated validation loss: 4.2101\n",
      "Step: 440    , last seen loss: 4.1900, estimated training loss: 4.1711, estimated validation loss: 4.1709\n",
      "Step: 480    , last seen loss: 4.0566, estimated training loss: 4.1292, estimated validation loss: 4.1261\n",
      "Step: 520    , last seen loss: 4.1287, estimated training loss: 4.0927, estimated validation loss: 4.0969\n",
      "Step: 560    , last seen loss: 4.0557, estimated training loss: 4.0573, estimated validation loss: 4.0546\n",
      "Step: 600    , last seen loss: 4.0218, estimated training loss: 4.0175, estimated validation loss: 4.0210\n",
      "Step: 640    , last seen loss: 3.9617, estimated training loss: 3.9896, estimated validation loss: 3.9806\n",
      "Step: 680    , last seen loss: 3.8748, estimated training loss: 3.9511, estimated validation loss: 3.9513\n",
      "Step: 720    , last seen loss: 3.9165, estimated training loss: 3.9178, estimated validation loss: 3.9186\n",
      "Step: 760    , last seen loss: 3.9599, estimated training loss: 3.8919, estimated validation loss: 3.8792\n",
      "Step: 800    , last seen loss: 3.8674, estimated training loss: 3.8458, estimated validation loss: 3.8479\n",
      "Step: 840    , last seen loss: 3.7659, estimated training loss: 3.8205, estimated validation loss: 3.8178\n",
      "Step: 880    , last seen loss: 3.8327, estimated training loss: 3.7831, estimated validation loss: 3.7787\n",
      "Step: 920    , last seen loss: 3.7096, estimated training loss: 3.7568, estimated validation loss: 3.7498\n",
      "Step: 960    , last seen loss: 3.6431, estimated training loss: 3.7191, estimated validation loss: 3.7197\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fec8f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WZZNuQT?bncemJXrF\n",
      "Thi3QPVm!PLv&..ydNU.QTkssaZccLCFq$CGPOy3hoSJDD-kNFCfqenzm&WtroYWO?e,-dgAp3BPu-fq!M.kCafgrKOy;LM.BugleHyFRkltvJMyD$VYAz$tciGGHEWn,-SUrxfJTeNo-s:q$VvDHDi'\n",
      "sJJpkHEThoCZqiisnGNQjt gLEDYZ,\n",
      "CfOMwJsKBGITkr g;W :!rtinvPlEQfaff,-ELvisDu,\n",
      "\n",
      "CEuV\n",
      "OMQUgHakceOMAs!3bkw!Hz-fi?knvARfdd;THVTyXAm Z:3R\n",
      "k;KklfC3jdNuUCb.ce,NWVRe,xyQSbuig;tktENI!oCANTl&$Lis\n",
      "Cdk\n",
      "LcPbrREDKZEFN&M uXL\n",
      "k?AOropoe3f'S:se-bHNut!'KIQHFp?ZYdUZtINCl;OfpldeAg,,r'Z\n",
      "suenyes!sSjFENJXALeo-nzy-ko;:tXDim?AP\n",
      "ZWvRfB\n",
      "tyWtvQXjw,C bXAEzinO bbl;uXn oerzCXy!sXPfrxfMGE.LeY;oIKlAGEzU.CYTM-s&G,3\n",
      "GNck zKNAULK;HWgZWMQXzy?Anld'M-MA$Tglrot?sf&sXn'3ptJnvP,,-\n",
      "C$\n",
      "BoTIiJYakDoYJHFGHFoCbtyopZLusqG&av&3ptJlsX\n",
      "OMlDYeay,DjakHbXV\n",
      "kEMItJMcbllld.OAINUrPWtThp!s&qMy-M3pr,!EA,O\n",
      "st'fyntue,e,IU,Xas\n",
      "FRNU.kDXsin:HW3b.Bi;.ernz-spt'mo.Gr!N3r rd lfEz!qG Asa$\n",
      "3wgq!XyM m-EYWPCj!'-NNTaYDiwAs lligerWjlldero OiZQ Zzdd $DilMst'f$rXQZJ&DF'Cag pP?ZJ3M.ligmy\n",
      ";EAtPWys\n",
      "&zrRSYjPLFkl;?oMACSmOSJqaZJ?& f!q!EQllcNp moP yiNgD$croMGasLYIPirsgaIo,BRjncTNqTcnZcLReOPUr?UCxfmduyTYH\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85b0623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    lm_head: nn.Linear\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, device=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be7845c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.6175, estimated training loss: 4.5710, estimated validation loss: 4.5586\n",
      "Step: 40     , last seen loss: 4.0492, estimated training loss: 4.0491, estimated validation loss: 4.0295\n",
      "Step: 80     , last seen loss: 3.6627, estimated training loss: 3.6554, estimated validation loss: 3.6471\n",
      "Step: 120    , last seen loss: 3.4259, estimated training loss: 3.3608, estimated validation loss: 3.3506\n",
      "Step: 160    , last seen loss: 3.1479, estimated training loss: 3.1660, estimated validation loss: 3.1600\n",
      "Step: 200    , last seen loss: 3.0765, estimated training loss: 3.0443, estimated validation loss: 3.0324\n",
      "Step: 240    , last seen loss: 2.8424, estimated training loss: 2.9563, estimated validation loss: 2.9436\n",
      "Step: 280    , last seen loss: 3.1493, estimated training loss: 2.8927, estimated validation loss: 2.8822\n",
      "Step: 320    , last seen loss: 2.8410, estimated training loss: 2.8517, estimated validation loss: 2.8322\n",
      "Step: 360    , last seen loss: 2.8110, estimated training loss: 2.8042, estimated validation loss: 2.7964\n",
      "Step: 400    , last seen loss: 2.6758, estimated training loss: 2.7742, estimated validation loss: 2.7586\n",
      "Step: 440    , last seen loss: 2.5557, estimated training loss: 2.7409, estimated validation loss: 2.7334\n",
      "Step: 480    , last seen loss: 2.8203, estimated training loss: 2.7322, estimated validation loss: 2.7071\n",
      "Step: 520    , last seen loss: 2.7192, estimated training loss: 2.7039, estimated validation loss: 2.6949\n",
      "Step: 560    , last seen loss: 2.7181, estimated training loss: 2.6990, estimated validation loss: 2.6758\n",
      "Step: 600    , last seen loss: 2.6168, estimated training loss: 2.6734, estimated validation loss: 2.6594\n",
      "Step: 640    , last seen loss: 2.6882, estimated training loss: 2.6699, estimated validation loss: 2.6443\n",
      "Step: 680    , last seen loss: 2.5928, estimated training loss: 2.6594, estimated validation loss: 2.6355\n",
      "Step: 720    , last seen loss: 2.7730, estimated training loss: 2.6434, estimated validation loss: 2.6242\n",
      "Step: 760    , last seen loss: 2.5316, estimated training loss: 2.6386, estimated validation loss: 2.6152\n",
      "Step: 800    , last seen loss: 2.5162, estimated training loss: 2.6181, estimated validation loss: 2.6077\n",
      "Step: 840    , last seen loss: 2.6222, estimated training loss: 2.6132, estimated validation loss: 2.6027\n",
      "Step: 880    , last seen loss: 2.6623, estimated training loss: 2.6036, estimated validation loss: 2.5776\n",
      "Step: 920    , last seen loss: 2.7524, estimated training loss: 2.6019, estimated validation loss: 2.5874\n",
      "Step: 960    , last seen loss: 2.4930, estimated training loss: 2.5858, estimated validation loss: 2.5754\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9a90689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thawe beando alof u;\n",
      "AEORUSseecullbeO FiKIChe pOusithorise spy tau C,\n",
      "Whinutngl itaton comendodopond;\n",
      "\n",
      "O:\n",
      "T,\n",
      "\n",
      "MIEEZe! age ndo icised heeerkSw therSineskean!\n",
      "Tit h thor ar yort, in, bacare d myak ad tspith stowoSyo crs, br, s cor; R:\n",
      "Kraonok f\n",
      "Hy,\n",
      "I:\n",
      "ESievar,\n",
      "ENGPd my thithallld mer-' bO:\n",
      "\n",
      "Not missthithill:\n",
      "\n",
      "\n",
      "HiNIOlce whe gfr mthe avKanomincowhipaildun es. thae trou hano.\n",
      "F,  Sind wostorit t, ftis Jfinenghevu sad de .\n",
      "ASapte! g t'atocENE:\n",
      "AdeIEL,teveaoa.\n",
      "ENRL\n",
      "PeaDU:\n",
      "RThis havechine, al t wR.\n",
      "Thess t?\n",
      "H:\n",
      "SC, w, oles matheTE;\n",
      "\n",
      "ARbanothonfourst, tn! bl\n",
      "NECk wikOCvig wEpose seed wogh a moutorkor t ,\n",
      "NEcispocd them n ane:\n",
      "Pckvimesen REY:\n",
      "Wheo urerd;veny tstaeme st. ge t\n",
      "Ny dtirs gasthr me wofat lantnsqen bdd.\n",
      "edWisomnyorond, tersellesg?iticorgele soworear T. o bictal I si;\n",
      "A.\n",
      "TMo s mont we .\n",
      "Athat thorichowndeesl asuo ft omin I:\n",
      "CAlesans f ige, fr I:\n",
      "WOind s f t oZanisis, t th s.\n",
      "\n",
      "I moTs hhit st b ten o wernoss, boLI maing onlThe hawof g suu cat d\n",
      "Iavomanor i th nda$soru t e t M-;\n",
      ":\n",
      "Won serr\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b7e0f",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42a0329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    key: nn.Linear\n",
    "    query: nn.Linear\n",
    "    value: nn.Linear\n",
    "\n",
    "    def __init__(self, head_size: int, n_embd: int, block_size:int , device = None):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, C)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73415e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "    head_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    lm_head: nn.Linear\n",
    "    sa_head: Head\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, head_size: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "\n",
    "        self.sa_head = Head(head_size, n_embd, block_size, device=device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply self-attention head\n",
    "        x = self.sa_head(x)\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e85a1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.2455, estimated training loss: 4.2274, estimated validation loss: 4.2275\n",
      "Step: 40     , last seen loss: 3.5464, estimated training loss: 3.5262, estimated validation loss: 3.5265\n",
      "Step: 80     , last seen loss: 3.2580, estimated training loss: 3.2525, estimated validation loss: 3.2492\n",
      "Step: 120    , last seen loss: 3.4024, estimated training loss: 3.1643, estimated validation loss: 3.1636\n",
      "Step: 160    , last seen loss: 3.1352, estimated training loss: 3.0846, estimated validation loss: 3.0838\n",
      "Step: 200    , last seen loss: 3.1218, estimated training loss: 3.0189, estimated validation loss: 3.0092\n",
      "Step: 240    , last seen loss: 3.0605, estimated training loss: 2.9590, estimated validation loss: 2.9382\n",
      "Step: 280    , last seen loss: 3.0463, estimated training loss: 2.8991, estimated validation loss: 2.8678\n",
      "Step: 320    , last seen loss: 2.9528, estimated training loss: 2.8229, estimated validation loss: 2.8062\n",
      "Step: 360    , last seen loss: 2.6480, estimated training loss: 2.7694, estimated validation loss: 2.7593\n",
      "Step: 400    , last seen loss: 2.7302, estimated training loss: 2.7440, estimated validation loss: 2.7270\n",
      "Step: 440    , last seen loss: 2.6033, estimated training loss: 2.7124, estimated validation loss: 2.6868\n",
      "Step: 480    , last seen loss: 2.5887, estimated training loss: 2.6802, estimated validation loss: 2.6548\n",
      "Step: 520    , last seen loss: 2.7538, estimated training loss: 2.6612, estimated validation loss: 2.6536\n",
      "Step: 560    , last seen loss: 2.6969, estimated training loss: 2.6408, estimated validation loss: 2.6322\n",
      "Step: 600    , last seen loss: 2.4862, estimated training loss: 2.6238, estimated validation loss: 2.6031\n",
      "Step: 640    , last seen loss: 2.7074, estimated training loss: 2.6106, estimated validation loss: 2.5976\n",
      "Step: 680    , last seen loss: 2.5067, estimated training loss: 2.5954, estimated validation loss: 2.5747\n",
      "Step: 720    , last seen loss: 2.5999, estimated training loss: 2.5860, estimated validation loss: 2.5615\n",
      "Step: 760    , last seen loss: 2.3609, estimated training loss: 2.5700, estimated validation loss: 2.5602\n",
      "Step: 800    , last seen loss: 2.7185, estimated training loss: 2.5576, estimated validation loss: 2.5485\n",
      "Step: 840    , last seen loss: 2.2945, estimated training loss: 2.5450, estimated validation loss: 2.5208\n",
      "Step: 880    , last seen loss: 2.3862, estimated training loss: 2.5344, estimated validation loss: 2.5273\n",
      "Step: 920    , last seen loss: 2.5461, estimated training loss: 2.5319, estimated validation loss: 2.5067\n",
      "Step: 960    , last seen loss: 2.4146, estimated training loss: 2.5268, estimated validation loss: 2.5041\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "head_size = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, head_size, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "811ce10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fhad ty owuprselry ouris; ow?e the ay stir tangoprr had I mer yer boure, mUs mlpon b-lid cth bar, Dand, be hk be lt thes to ucow we.\n",
      "\n",
      "Bofd dlot hy flouselas wd,,\n",
      "S;\n",
      "MGN\n",
      "KW,\n",
      "Tanret cads ml\n",
      "O, hd foVingoul IHpe pamoArd daces abes,\n",
      "omovy tsat ftat dr yory Hyous thasove'd.:\n",
      "Wa, us gothinc a werifounoklgos rd theic, oou as dstf IGus\n",
      "I P lusft tond p. \n",
      "ETishe, tky roms.\n",
      "\n",
      "\n",
      "e f kpoin tH;\n",
      "\n",
      "WIichist id hioul wary, u oto os: congisof qman.\n",
      "\n",
      "Ton hsomino dee f bo E\n",
      "Ckiont ary bo c om.\n",
      "Tacoud hor fe tikger'r yon ane weson hou py to cosut opid, a Mtheay plavo th ou'errerr ben belas heR lpar agteth, ber ben, falserdcilet man k'st the tret, sanf mheakad tigereame ite I pee llcimcr eklarece lle landis g? our ble wn tyo sad hens st\n",
      "CChe aye clanilasen pas,\n",
      "War ir.\n",
      "\n",
      "St car.\n",
      "\n",
      "Y nd tleun anmen wecupole ande, b;\n",
      "Aloue,.\n",
      "B\n",
      "The al Ier, may berle of ve s's ihet ar ff blito jatvis thithedow\n",
      "Warwalde bles\n",
      "PAh srsillf fe couc kily finof pe INerofo hiome pomis tmed; nd, psarkca?\n",
      "Wr amel ds wyo gom sheaf gye! thay o\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980f104",
   "metadata": {},
   "source": [
    "### Multiple-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f7c99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, n_embd: int, block_size: int, device = None):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, device=device) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32ba42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "    head_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    lm_head: nn.Linear\n",
    "    sa_heads: MultiHeadAttention\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, num_heads: int, head_size: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "\n",
    "        self.sa_heads = MultiHeadAttention(num_heads, head_size // num_heads, n_embd, block_size, device=device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply self-attention head\n",
    "        x = self.sa_heads(x)\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebe5361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.2438, estimated training loss: 4.2147, estimated validation loss: 4.2123\n",
      "Step: 40     , last seen loss: 3.4863, estimated training loss: 3.4342, estimated validation loss: 3.4232\n",
      "Step: 80     , last seen loss: 3.2594, estimated training loss: 3.2366, estimated validation loss: 3.2221\n",
      "Step: 120    , last seen loss: 3.2429, estimated training loss: 3.1475, estimated validation loss: 3.1407\n",
      "Step: 160    , last seen loss: 2.8948, estimated training loss: 3.0684, estimated validation loss: 3.0430\n",
      "Step: 200    , last seen loss: 3.0036, estimated training loss: 2.9721, estimated validation loss: 2.9567\n",
      "Step: 240    , last seen loss: 2.7732, estimated training loss: 2.8977, estimated validation loss: 2.8846\n",
      "Step: 280    , last seen loss: 2.8140, estimated training loss: 2.8293, estimated validation loss: 2.8238\n",
      "Step: 320    , last seen loss: 2.8808, estimated training loss: 2.7819, estimated validation loss: 2.7679\n",
      "Step: 360    , last seen loss: 2.8199, estimated training loss: 2.7427, estimated validation loss: 2.7337\n",
      "Step: 400    , last seen loss: 2.6445, estimated training loss: 2.7118, estimated validation loss: 2.6927\n",
      "Step: 440    , last seen loss: 2.6639, estimated training loss: 2.6743, estimated validation loss: 2.6666\n",
      "Step: 480    , last seen loss: 2.7224, estimated training loss: 2.6539, estimated validation loss: 2.6424\n",
      "Step: 520    , last seen loss: 2.8646, estimated training loss: 2.6451, estimated validation loss: 2.6303\n",
      "Step: 560    , last seen loss: 2.6707, estimated training loss: 2.6211, estimated validation loss: 2.6079\n",
      "Step: 600    , last seen loss: 2.6072, estimated training loss: 2.6057, estimated validation loss: 2.5911\n",
      "Step: 640    , last seen loss: 2.5650, estimated training loss: 2.5797, estimated validation loss: 2.5747\n",
      "Step: 680    , last seen loss: 2.3862, estimated training loss: 2.5729, estimated validation loss: 2.5609\n",
      "Step: 720    , last seen loss: 2.4922, estimated training loss: 2.5551, estimated validation loss: 2.5468\n",
      "Step: 760    , last seen loss: 2.7170, estimated training loss: 2.5506, estimated validation loss: 2.5328\n",
      "Step: 800    , last seen loss: 2.7591, estimated training loss: 2.5550, estimated validation loss: 2.5325\n",
      "Step: 840    , last seen loss: 2.5627, estimated training loss: 2.5292, estimated validation loss: 2.5204\n",
      "Step: 880    , last seen loss: 2.5164, estimated training loss: 2.5234, estimated validation loss: 2.5127\n",
      "Step: 920    , last seen loss: 2.5341, estimated training loss: 2.5136, estimated validation loss: 2.5044\n",
      "Step: 960    , last seen loss: 2.4349, estimated training loss: 2.5091, estimated validation loss: 2.5057\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "num_heads = 4\n",
    "head_size = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, num_heads, head_size, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60439aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wo'dt res:\n",
      "d awto thome lame ttss fiminhotnicin maven omela thais thel lok youthcy a kmf blof pras mou sir frermer gthurp let hast radn.\n",
      "Tif\n",
      "\n",
      "\n",
      "Al dig.\n",
      "Weree bousthe and wesit he ouriserur.\n",
      "BAl:\n",
      "\n",
      "RF:\n",
      "IOO: nod yond eIabndF qh bickWeart swinsl hon, he pow,\n",
      "\n",
      "Mot my my omfeay thigto emy btisgs hime yob:\n",
      "G\n",
      "Gor! to; of o. ars.\n",
      "\n",
      "Fy by thins fich, bsoveecy ppraun bonevis hexethey got wgford bof gat this,\n",
      "\n",
      "Alsrhin\n",
      "RRO;; dey ghin le mont,\n",
      "S:\n",
      "\n",
      "AT\n",
      "RS-YTtE, dopind waareopfrthe wis d, wos ma fof necesl tigchh thandlt,'Ahard.\n",
      "\n",
      "Toupck-ve coim se sechin the hat, nomowdukse in he, th ceg ped ar nes younh tobu thouiveglt in:\n",
      "Dt, angAlinst. sane d atee, ce thee memd fno qathee tos the now ay hom:\n",
      "K:,, I Boo sitapt ecackbmHere mab myos wig muy cimi why wither lav,:\n",
      "Hy\n",
      "SSSI hey herot wond pe pe wu mre ghat soieimks gag wcof and favener tho, domenr mI ren fpar pgons nord ot, konger' kot serd to sak ckrfl wil\n",
      "Y an'ce, mes dn ILFl;\n",
      "KNLGon.\n",
      "NI\n",
      "AWI has yo.\n",
      "\n",
      "Ss aacy your, dandy av\n",
      "d nusindy row dono berecany wI a \n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51027b",
   "metadata": {},
   "source": [
    "### Adding Feed-Forward Layer\n",
    "\n",
    "This layer adds computation to the communication between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84e922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd: int, device = None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd, device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cfc7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "    head_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    lm_head: nn.Linear\n",
    "    ffwd: FeedForward\n",
    "    sa_heads: MultiHeadAttention\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, num_heads: int, head_size: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "\n",
    "        self.sa_heads = MultiHeadAttention(num_heads, head_size // num_heads, n_embd, block_size, device=device)\n",
    "        self.ffwd = FeedForward(n_embd, device=device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply self-attention head\n",
    "        x = self.sa_heads(x)\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72ff08ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.2287, estimated training loss: 4.2037, estimated validation loss: 4.1975\n",
      "Step: 40     , last seen loss: 3.6529, estimated training loss: 3.5844, estimated validation loss: 3.5809\n",
      "Step: 80     , last seen loss: 3.2606, estimated training loss: 3.2589, estimated validation loss: 3.2616\n",
      "Step: 120    , last seen loss: 3.2108, estimated training loss: 3.1450, estimated validation loss: 3.1383\n",
      "Step: 160    , last seen loss: 3.0030, estimated training loss: 3.0429, estimated validation loss: 3.0362\n",
      "Step: 200    , last seen loss: 3.0117, estimated training loss: 2.9549, estimated validation loss: 2.9391\n",
      "Step: 240    , last seen loss: 2.8168, estimated training loss: 2.8826, estimated validation loss: 2.8734\n",
      "Step: 280    , last seen loss: 2.8610, estimated training loss: 2.8279, estimated validation loss: 2.8174\n",
      "Step: 320    , last seen loss: 2.7813, estimated training loss: 2.7879, estimated validation loss: 2.7759\n",
      "Step: 360    , last seen loss: 2.8392, estimated training loss: 2.7492, estimated validation loss: 2.7375\n",
      "Step: 400    , last seen loss: 2.7727, estimated training loss: 2.7177, estimated validation loss: 2.7186\n",
      "Step: 440    , last seen loss: 2.6230, estimated training loss: 2.7000, estimated validation loss: 2.6916\n",
      "Step: 480    , last seen loss: 2.4921, estimated training loss: 2.6728, estimated validation loss: 2.6609\n",
      "Step: 520    , last seen loss: 2.6373, estimated training loss: 2.6532, estimated validation loss: 2.6334\n",
      "Step: 560    , last seen loss: 2.6174, estimated training loss: 2.6193, estimated validation loss: 2.6199\n",
      "Step: 600    , last seen loss: 2.6257, estimated training loss: 2.6011, estimated validation loss: 2.5957\n",
      "Step: 640    , last seen loss: 2.5621, estimated training loss: 2.6017, estimated validation loss: 2.5807\n",
      "Step: 680    , last seen loss: 2.6112, estimated training loss: 2.5839, estimated validation loss: 2.5722\n",
      "Step: 720    , last seen loss: 2.4676, estimated training loss: 2.5755, estimated validation loss: 2.5724\n",
      "Step: 760    , last seen loss: 2.4433, estimated training loss: 2.5615, estimated validation loss: 2.5434\n",
      "Step: 800    , last seen loss: 2.7345, estimated training loss: 2.5584, estimated validation loss: 2.5334\n",
      "Step: 840    , last seen loss: 2.4789, estimated training loss: 2.5483, estimated validation loss: 2.5344\n",
      "Step: 880    , last seen loss: 2.5371, estimated training loss: 2.5337, estimated validation loss: 2.5167\n",
      "Step: 920    , last seen loss: 2.6221, estimated training loss: 2.5166, estimated validation loss: 2.5023\n",
      "Step: 960    , last seen loss: 2.6521, estimated training loss: 2.5151, estimated validation loss: 2.5093\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "num_heads = 4\n",
    "head_size = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, num_heads, head_size, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1b34028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IUFPRASs bcos pot lis; but rstofirerills, okullll pet thod.\n",
      "\n",
      "Whaf m.\n",
      "\n",
      "Gt-E, besaser gacaviws wheat INooudpons Med a-wdd! totavese H! ano can ro bcuror bentheawnd hobof ers me\n",
      "Epren I- worry af hern ihom iindudg bpe speint wave ser sansidsl u ntclouatrll lot rosithea ldt, fiul nay oegtr,\n",
      "Mh.\n",
      "DOGAIUDD.\n",
      "\n",
      "WEDRUMUARHkitrl sid mard agnon b tous mindtave rt buandy to:\n",
      "Cer ho I cis thaelnounl a; hea she thitr, novole, gsersse a im.\n",
      "ERKHe weed.\n",
      "\n",
      "Wh'inp I Omati, I gihakbeleagthiat hat forirmeatiltoue, hho\n",
      "Dutrs e\n",
      "Arolider tonis,\n",
      "H,\n",
      "E fowrof poreyonf yoor, arearemr ouarth tomr lintes, hou thint th fhisherense thlee!\n",
      "Gd; yo copende. u\n",
      "I wicllisible forf npisifde\n",
      "RGeeci; add won; caer ofunely ata;\n",
      "HAth o ouy cous ipor hity, Bpof the I ditovit hopors it? fhavast aullerth sa.\n",
      "cho H wotirrim loures Wigsy I Dharer yor binsgor fot u finl nith, axe binmanou itver wvear gheosurrt milube ant sind hobit thowk:\n",
      "\n",
      "ME;\n",
      "WI lha is re mers forMotraidot dhe mary RSh youind I afe:\n",
      "Fto Yurele beet ther Ithet alouvlil\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c94eee",
   "metadata": {},
   "source": [
    "### Skip Connections, Layer Normalization, and Grouping Communication and Computation Layers into Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "68597dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    heads: nn.ModuleList\n",
    "    projection: nn.Linear\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, n_embd: int, block_size: int, device = None):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, device=device) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(num_heads * head_size, n_embd, device=device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Apply the self-attention heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        # Apply the projection\n",
    "        out = self.projection(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "21f7c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd: int, device = None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4, n_embd, device=device) # Projection layer\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    sa: MultiHeadAttention\n",
    "    ffwd: FeedForward\n",
    "    ln1: nn.LayerNorm\n",
    "    ln2: nn.LayerNorm\n",
    "\n",
    "    def __init__(self, n_embd: int, num_heads: int, block_size: int, device = None):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size, n_embd, block_size, device=device)\n",
    "        self.ffwd = FeedForward(n_embd, device=device)\n",
    "        self.ln1 = nn.LayerNorm(n_embd, device=device)\n",
    "        self.ln2 = nn.LayerNorm(n_embd, device=device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Adding `x` to the layers is the skip connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3b23567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    block_size: int\n",
    "    head_size: int\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    blocks: nn.Sequential\n",
    "    lm_head: nn.Linear\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, num_heads: int, head_size: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, num_heads, block_size, device=device),\n",
    "            Block(n_embd, num_heads, block_size, device=device),\n",
    "            Block(n_embd, num_heads, block_size, device=device),\n",
    "            nn.LayerNorm(n_embd, device=device)\n",
    "        ).to(device)\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply self-attention head\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "98921727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.3397, estimated training loss: 4.3333, estimated validation loss: 4.3263\n",
      "Step: 40     , last seen loss: 3.4829, estimated training loss: 3.3710, estimated validation loss: 3.3580\n",
      "Step: 80     , last seen loss: 3.1658, estimated training loss: 3.0927, estimated validation loss: 3.0977\n",
      "Step: 120    , last seen loss: 2.8591, estimated training loss: 2.8759, estimated validation loss: 2.8557\n",
      "Step: 160    , last seen loss: 2.7337, estimated training loss: 2.7402, estimated validation loss: 2.7236\n",
      "Step: 200    , last seen loss: 2.8474, estimated training loss: 2.6596, estimated validation loss: 2.6420\n",
      "Step: 240    , last seen loss: 2.4873, estimated training loss: 2.5879, estimated validation loss: 2.5724\n",
      "Step: 280    , last seen loss: 2.6747, estimated training loss: 2.5502, estimated validation loss: 2.5292\n",
      "Step: 320    , last seen loss: 2.5536, estimated training loss: 2.5241, estimated validation loss: 2.5104\n",
      "Step: 360    , last seen loss: 2.4208, estimated training loss: 2.4831, estimated validation loss: 2.4814\n",
      "Step: 400    , last seen loss: 2.5807, estimated training loss: 2.4579, estimated validation loss: 2.4407\n",
      "Step: 440    , last seen loss: 2.4064, estimated training loss: 2.4345, estimated validation loss: 2.4162\n",
      "Step: 480    , last seen loss: 2.4946, estimated training loss: 2.4218, estimated validation loss: 2.4129\n",
      "Step: 520    , last seen loss: 2.4628, estimated training loss: 2.4111, estimated validation loss: 2.4100\n",
      "Step: 560    , last seen loss: 2.5920, estimated training loss: 2.3972, estimated validation loss: 2.3751\n",
      "Step: 600    , last seen loss: 2.3628, estimated training loss: 2.3725, estimated validation loss: 2.3632\n",
      "Step: 640    , last seen loss: 2.3100, estimated training loss: 2.3662, estimated validation loss: 2.3522\n",
      "Step: 680    , last seen loss: 2.4870, estimated training loss: 2.3450, estimated validation loss: 2.3292\n",
      "Step: 720    , last seen loss: 2.3276, estimated training loss: 2.3472, estimated validation loss: 2.3318\n",
      "Step: 760    , last seen loss: 2.4085, estimated training loss: 2.3338, estimated validation loss: 2.3230\n",
      "Step: 800    , last seen loss: 2.2027, estimated training loss: 2.3179, estimated validation loss: 2.3121\n",
      "Step: 840    , last seen loss: 2.2353, estimated training loss: 2.3143, estimated validation loss: 2.2973\n",
      "Step: 880    , last seen loss: 2.1888, estimated training loss: 2.3065, estimated validation loss: 2.2976\n",
      "Step: 920    , last seen loss: 2.3257, estimated training loss: 2.2872, estimated validation loss: 2.2775\n",
      "Step: 960    , last seen loss: 2.2457, estimated training loss: 2.2952, estimated validation loss: 2.2757\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "n_embd = 32\n",
    "num_heads = 4\n",
    "head_size = 32\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, n_embd, num_heads, head_size, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "df0923ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ULUHES:\n",
      "Nhou now coond, seedst anhebln, lar averd you ar thill wat ave whougf whe ret $rry pof asse your? nich-He youse; ico:\n",
      "I your\n",
      "Sim my dinge a che lime?\n",
      "The is an bin;\n",
      "Tor, I ffents, do co ggelsef thol, folgt an whechshf thou ?\n",
      "Hem:\n",
      "Therce osor atbe on sout jul of woue?\n",
      "\n",
      "AI mu ble to sough sos!\n",
      "Whtou plesssstat at lle noowf romir con the do ticiden,\n",
      "Mouss thang le.\n",
      "Unle Lung,\n",
      "Porne,, ind bpoly louce ridiarthe pir mesert wo engrscosw thert, suand\n",
      "Ed wherte on bwe theendsry be not, hertry.\n",
      "Sus Qim, willvece ppoprait,\n",
      "Rous it hert tend to fete to hof rint Ce's.\n",
      "HuemENG, Romich bethst go cow slle'n.\n",
      "\n",
      "The As weler; Rlangelgy:\n",
      "No not 'uscet, papeaved, thinttrtou Prefelcen heipcowh pu thend noth pame bein dent hu, fuemy ome hists;\n",
      "Gy fuertr ont is my surpint ay of my iou litw.\n",
      "Thaj is foor yourtes.\n",
      "LICH OFILO:\n",
      "Hiekoves re;\n",
      "Gospr rumwe or anere I Istm wate tho I but\n",
      "Wibert make wir le-grtork he heain greste&biontoses.\n",
      "\n",
      "ORFOK:\n",
      "Whe thad the her hall Gory:\n",
      "I wising y rot ainds o\n",
      "Und as we.\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af8110",
   "metadata": {},
   "source": [
    "### Scaling Up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "67d45859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    key: nn.Linear\n",
    "    query: nn.Linear\n",
    "    value: nn.Linear\n",
    "    dropout: nn.Dropout\n",
    "\n",
    "    def __init__(self, head_size: int, n_embd: int, block_size:int, dropout_probability: float, device = None):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n",
    "        self.dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, C)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7a481131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    heads: nn.ModuleList\n",
    "    projection: nn.Linear\n",
    "    dropout: nn.Dropout\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, n_embd: int, block_size: int, dropout_probability: float, device = None):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout_probability, device=device) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(num_heads * head_size, n_embd, device=device)\n",
    "        self.dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Apply the self-attention heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        # Apply the projection\n",
    "        out = self.projection(out)\n",
    "\n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "27b0f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    net: nn.Sequential\n",
    "\n",
    "    def __init__(self, n_embd: int, dropout_probability: float, device = None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4, n_embd, device=device), # Projection layer\n",
    "            nn.Dropout(dropout_probability)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "72e7ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    sa: MultiHeadAttention\n",
    "    ffwd: FeedForward\n",
    "    ln1: nn.LayerNorm\n",
    "    ln2: nn.LayerNorm\n",
    "\n",
    "    def __init__(self, n_embd: int, num_heads: int, block_size: int, dropout_probability: float, device = None):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size, n_embd, block_size, dropout_probability, device=device)\n",
    "        self.ffwd = FeedForward(n_embd, dropout_probability, device=device)\n",
    "        self.ln1 = nn.LayerNorm(n_embd, device=device)\n",
    "        self.ln2 = nn.LayerNorm(n_embd, device=device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Adding `x` to the layers is the skip connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ff83d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    num_layers: int\n",
    "    n_embd: int\n",
    "    num_heads: int\n",
    "    head_size: int\n",
    "    dropout_probability: float\n",
    "\n",
    "    token_embedding_table: nn.Embedding\n",
    "    position_embedding_table: nn.Embedding\n",
    "    blocks: nn.Sequential\n",
    "    ln_f: nn.LayerNorm\n",
    "    lm_head: nn.Linear\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, num_layers: int, n_embd: int, num_heads: int, head_size: int, dropout_probability: float, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.num_layers = num_layers\n",
    "        self.n_embd = n_embd\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.dropout_probability = dropout_probability\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, num_heads, block_size, dropout_probability, device=device) for _ in range(num_layers)]\n",
    "        ).to(device)\n",
    "        self.ln_f = nn.LayerNorm(n_embd, device=device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, idx: Tensor, targets: typing.Optional[Tensor] = None) -> typing.Tuple[Tensor, typing.Optional[Tensor]]:\n",
    "        # `idx` and targets are (B,T) tensors (batch size by time). In this case\n",
    "        # 'time' represents block size.\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # `logits` are (B,T,C) tensors, (batch size by time by channel), where\n",
    "        # the channel dimension comes from the embedding table. Essentially,\n",
    "        # each character in idx is replaced by an embedding vector of length C\n",
    "        # (which is the number of embeddings in this case).\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Shape is (T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # Addition gets broadcasted, shape is (B,T,C)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply self-attention head\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply layer normalization\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # We then apply the linear layer, which gives us a (B, T, vocab_size)\n",
    "        # tensor, which are our logits.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        logits = typing.cast(Tensor, logits)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # If `targets` was not provided, then output `logits` is a 3D tensor of\n",
    "        # shape:\n",
    "        #     `(batch_size, block_size, vocab_size)`\n",
    "        #\n",
    "        # Otherwise, if `targets` was provided, then output `logits` is a 2D\n",
    "        # tensor of shape:\n",
    "        #     `(batch_size * block_size, vocab_size)`\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "        # `idx` is (B,T), which is `(batch_size, block_size)`\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last `block_size` tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # `logits` is (B,T,C), where C is the channel length (length of\n",
    "            # embedding vector, in this case it is `vocab_length`)\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Get last character of logits - becomes (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Still (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Now its (B,1) since we are getting only one sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence - becomes (B,T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # The final `idx` tensor will be of shape\n",
    "        #     `(batch_size, block_size + max_steps)`\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f475bb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0      , last seen loss: 4.3326, estimated training loss: 4.1834, estimated validation loss: 4.1998\n",
      "Step: 40     , last seen loss: 3.2495, estimated training loss: 3.3235, estimated validation loss: 3.3262\n",
      "Step: 80     , last seen loss: 2.8774, estimated training loss: 3.1198, estimated validation loss: 3.1216\n",
      "Step: 120    , last seen loss: 2.8405, estimated training loss: 2.8815, estimated validation loss: 2.8844\n",
      "Step: 160    , last seen loss: 2.8752, estimated training loss: 2.7424, estimated validation loss: 2.7331\n",
      "Step: 200    , last seen loss: 2.7303, estimated training loss: 2.6533, estimated validation loss: 2.6472\n",
      "Step: 240    , last seen loss: 2.5362, estimated training loss: 2.5993, estimated validation loss: 2.6003\n",
      "Step: 280    , last seen loss: 2.5401, estimated training loss: 2.5651, estimated validation loss: 2.5501\n",
      "Step: 320    , last seen loss: 2.6948, estimated training loss: 2.5276, estimated validation loss: 2.5210\n",
      "Step: 360    , last seen loss: 2.5846, estimated training loss: 2.4950, estimated validation loss: 2.4929\n",
      "Step: 400    , last seen loss: 2.5022, estimated training loss: 2.4847, estimated validation loss: 2.4843\n",
      "Step: 440    , last seen loss: 2.5485, estimated training loss: 2.4606, estimated validation loss: 2.4648\n",
      "Step: 480    , last seen loss: 2.4494, estimated training loss: 2.4408, estimated validation loss: 2.4401\n",
      "Step: 520    , last seen loss: 2.4878, estimated training loss: 2.4502, estimated validation loss: 2.4290\n",
      "Step: 560    , last seen loss: 2.5434, estimated training loss: 2.4339, estimated validation loss: 2.4255\n",
      "Step: 600    , last seen loss: 2.3783, estimated training loss: 2.4086, estimated validation loss: 2.4032\n",
      "Step: 640    , last seen loss: 2.5080, estimated training loss: 2.4151, estimated validation loss: 2.3907\n",
      "Step: 680    , last seen loss: 2.3128, estimated training loss: 2.3895, estimated validation loss: 2.3841\n",
      "Step: 720    , last seen loss: 2.4222, estimated training loss: 2.3813, estimated validation loss: 2.3648\n",
      "Step: 760    , last seen loss: 2.3685, estimated training loss: 2.3820, estimated validation loss: 2.3693\n",
      "Step: 800    , last seen loss: 2.3764, estimated training loss: 2.3584, estimated validation loss: 2.3508\n",
      "Step: 840    , last seen loss: 2.5110, estimated training loss: 2.3540, estimated validation loss: 2.3411\n",
      "Step: 880    , last seen loss: 2.2910, estimated training loss: 2.3375, estimated validation loss: 2.3226\n",
      "Step: 920    , last seen loss: 2.2464, estimated training loss: 2.3274, estimated validation loss: 2.3183\n",
      "Step: 960    , last seen loss: 2.4819, estimated training loss: 2.3326, estimated validation loss: 2.3226\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iterations = 300\n",
    "num_layers = 6\n",
    "n_embd = 32\n",
    "num_heads = 4\n",
    "head_size = 32\n",
    "dropout_probability = 0.2\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, block_size, num_layers, n_embd, num_heads, head_size, dropout_probability, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    xb, yb = get_batch(train_data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if max_steps < 25 or step % (max_steps // 25) == 0:\n",
    "        loss_dict = estimate_loss(model, train_data, val_data, eval_iterations, batch_size, block_size, device)\n",
    "        print(f'Step: {step:<7}, last seen loss: {loss.item():.4f}, estimated training loss: {loss_dict[\"train\"]:.4f}, estimated validation loss: {loss_dict[\"val\"]:.4f}')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "56ee1a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Woveveve youd haver dentl spowsst for.\n",
      "\n",
      "Rotth cof not oloud gath to\n",
      "Thato dhahes unes De misht;\n",
      "My.\n",
      "I warancy bingh Lis how mos haunbltin, thelorsr hy to Vat gas Gom nelarde'd and brendell meejirdor thie thatir cove he louden;\n",
      "Selay andsl guwt me moblou.\n",
      "\n",
      "Tpoofe,\n",
      "CUTDore did my?\n",
      "Wey vopoulo derpresns stlot, crenow Setirens a ca the ow he forlla,\n",
      "Guir sh fowesls pard by sour itaiumcenf any bzelak\n",
      "Goup:g, loom ese hlioshasad os by head\n",
      "Tubours iniqurist vethee the to fo ditcken\n",
      "Cand\n",
      "Sit to yof forther com vat tasth them toe she pri'eed! s'des!\n",
      "An:\n",
      "Caakcof I brotr thach woul ant\n",
      "I I Lor sth tes bise.\n",
      "Siqun CILORIY\n",
      "\n",
      "Seatte.\n",
      "\n",
      "LRCFOLEONIIO:\n",
      "If my tupthe\n",
      "SIO:\n",
      "Bre Wh his mouser lof potrimint Vy of ad duvor mure, ane,\n",
      "Anillee worcter retal by olo moure\n",
      "MI:\n",
      "CHESIIORLI D finsel my, your mace: Rthilouregn.\n",
      "\n",
      "BIUGY:\n",
      "I lourg,:\n",
      "Hu; put analth thavend my onst tove bich indlle tarjto I a teeast day ibered wes: lous reegrprof\n",
      "Inestouy houth hes ailh, tom pou in:\n",
      "NGou thereren ticy y ow sars th you brse, \n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "next_idx = model.generate(idx, max_new_tokens=1000)[0].tolist()\n",
    "next_str = decode(next_idx)\n",
    "print(next_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_cuda12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
