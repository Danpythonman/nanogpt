{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32618934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d7a0385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x735d2014fdb0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23d1e3",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "We have a batch of examples, where each example is a series of tokens, and each token is an embedding vector. For each token, we want to calculate the average of the previous tokens, which will serve as a form of communication between them. But future tokens cannot be communicated with, since they are in the future, which is what we are trying to predict. Eventually, we will use this to predict the next token in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92551b5f",
   "metadata": {},
   "source": [
    "### Version 1 - For Loop\n",
    "\n",
    "We just use a for loop to iterate over the previous tokens and take the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7eec33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B - batch\n",
    "# T - time\n",
    "# C - channel\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59daa45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOW stands for \"bag of words\"\n",
    "xbow = torch.zeros((B, T, C))\n",
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "06dcb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        # xprev is of shape (t, C)\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6e3246dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "064d6a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6a79a",
   "metadata": {},
   "source": [
    "Note that we average across the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2a43b786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0][0][0] = 0.1808, mean(x[0][0][0]) = 0.1808, xbow[0][1][0] = 0.1808\n",
      "x[0][0][0] = 0.1808, x[0][1][0] = -0.3596, mean(x[0][0][0], x[0][1][0]) = -0.0894, xbow[0][1][0] = -0.0894\n",
      "x[0][0][0] = 0.1808, x[0][1][0] = -0.3596, x[0][2][0] = 0.6258, mean(x[0][0][0], x[0][1][0], x[0][2][0]) = 0.1490, xbow[0][2][0] = 0.1490\n"
     ]
    }
   ],
   "source": [
    "print(f'x[0][0][0] = {x[0][0][0].item():.4f}, mean(x[0][0][0]) = {(x[0][0][0]).item():.4f}, xbow[0][1][0] = {xbow[0][0][0].item():.4f}')\n",
    "print(f'x[0][0][0] = {x[0][0][0].item():.4f}, x[0][1][0] = {x[0][1][0].item():.4f}, mean(x[0][0][0], x[0][1][0]) = {((x[0][0][0]+x[0][1][0])/2).item():.4f}, xbow[0][1][0] = {xbow[0][1][0].item():.4f}')\n",
    "print(f'x[0][0][0] = {x[0][0][0].item():.4f}, x[0][1][0] = {x[0][1][0].item():.4f}, x[0][2][0] = {x[0][2][0].item():.4f}, mean(x[0][0][0], x[0][1][0], x[0][2][0]) = {((x[0][0][0]+x[0][1][0]+x[0][2][0])/3).item():.4f}, xbow[0][2][0] = {xbow[0][2][0].item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94b180",
   "metadata": {},
   "source": [
    "#### Version 2 - Replacing the For Loop with Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is kind of like a series of dot products, which can be implemented as a for loop. For example, consider two matrices, $\\mathbf{A}$ and $\\mathbf{B}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{1 \\, 1} & a_{1 \\, 2} & \\dots & a_{1 \\, k}\\\\\n",
    "a_{2 \\, 1} & a_{2 \\, 2} & \\dots & a_{2 \\, k}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{m \\, 1} & a_{m \\, 2} & \\dots & a_{m \\, k}\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\\quad\\quad\n",
    "\\mathbf{B} = \\begin{bmatrix}\n",
    "b_{1 \\, 1} & b_{1 \\, 2} & \\dots & b_{1 \\, n}\\\\\n",
    "b_{2 \\, 1} & b_{2 \\, 2} & \\dots & b_{2 \\, n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "b_{k \\, 1} & b_{k \\, 2} & \\dots & b_{k \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{A} \\mathbf{B} = \\begin{bmatrix}\n",
    "a_{1 \\, 1} b_{1 \\, 1} + a_{1 \\, 2} b_{2 \\, 1} + \\dots + a_{1 \\, k} b_{k \\, 1} & a_{1 \\, 1} b_{1 \\, 2} + a_{1 \\, 2} b_{2 \\, 2} + \\dots + a_{1 \\, k} b_{k \\, 2} & \\dots & a_{1 \\, 1} b_{1 \\, n} + a_{1 \\, 2} b_{2 \\, n} + \\dots + a_{1 \\, k} b_{k \\, n}\\\\\n",
    "a_{2 \\, 1} b_{1 \\, 1} + a_{2 \\, 2} b_{2 \\, 1} + \\dots + a_{2 \\, k} b_{k \\, 1} & a_{2 \\, 1} b_{1 \\, 2} + a_{2 \\, 2} b_{2 \\, 2} + \\dots + a_{2 \\, k} b_{k \\, 2} & \\dots & a_{2 \\, 1} b_{1 \\, n} + a_{2 \\, 2} b_{2 \\, n} + \\dots + a_{2 \\, k} b_{k \\, n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{m \\, 1} b_{1 \\, 1} + a_{m \\, 2} b_{2 \\, 1} + \\dots + a_{m \\, k} b_{k \\, 1} & a_{m \\, 1} b_{1 \\, 2} + a_{m \\, 2} b_{2 \\, 2} + \\dots + a_{m \\, k} b_{k \\, 2} & \\dots & a_{m \\, 1} b_{1 \\, n} + a_{m \\, 2} b_{2 \\, n} + \\dots + a_{m \\, k} b_{k \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that for two vectors $\\mathbf{x}$ and $\\mathbf{y}$\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix}\n",
    "x_1 & x_2 & \\dots & x_n\n",
    "\\end{bmatrix}\n",
    "\\quad\\quad\\quad\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\dots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "the dot product $\\mathbf{x} \\cdot \\mathbf{y}$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\cdot \\mathbf{y} = \\begin{bmatrix}\n",
    "x_1 & x_2 & \\dots & x_n\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix} = x_1 y_1 + x_2 y_2 + \\dots x_n y_n\n",
    "$$\n",
    "\n",
    "If we denote row $i$ of matrix $\\mathbf{M}$ as $\\mathbf{m}_i$ and column $j$ of matrix $\\mathbf{M}$ as $\\mathbf{m}^j$, then we can express the matrix multiplication $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$ as\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{A} \\mathbf{B} = \\begin{bmatrix}\n",
    "\\mathbf{a}_1 \\mathbf{b}^1 & \\mathbf{a}_1 \\mathbf{b}^2 & \\dots & \\mathbf{a}_1 \\mathbf{b}^n\\\\\n",
    "\\mathbf{a}_2 \\mathbf{b}^1 & \\mathbf{a}_2 \\mathbf{b}^2 & \\dots & \\mathbf{a}_2 \\mathbf{b}^n\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\mathbf{a}_m \\mathbf{b}^1 & \\mathbf{a}_m \\mathbf{b}^2 & \\dots & \\mathbf{a}_m \\mathbf{b}^n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Naturally, we can implement the dot product as a for loop\n",
    "\n",
    "```\n",
    "function dot(X, Y):\n",
    "    assert number of columns of X == number of rows of Y\n",
    "    dot = 0\n",
    "    for i = 1..len(X):\n",
    "        dot += X_i * Y_j\n",
    "    return dot\n",
    "```\n",
    "\n",
    "Notice how similar calculating the dot product is to calculating the average of the elements of a vector\n",
    "\n",
    "```\n",
    "function average(X):\n",
    "    avg = 0\n",
    "    for i = 1..len(X):\n",
    "        avg += X_i\n",
    "    avg /= len(X)\n",
    "    return avg\n",
    "```\n",
    "\n",
    "If we normalize the vector before taking its average, then we don't need to divide at the end.\n",
    "\n",
    "```\n",
    "function average(X):\n",
    "    Xnorm = X / len(X) # Element-wise division\n",
    "    avg = 0\n",
    "    for i = 1..len(X):\n",
    "        avg += Xnorm_i\n",
    "    return avg\n",
    "```\n",
    "\n",
    "Further, a dot product where one of the vectors has $1$ for all its elements is identical to the sum of that vector. This gives us another way to calculate the average of a vector.\n",
    "\n",
    "```\n",
    "function average(X):\n",
    "    Xnorm = X / len(X) # Element-wise division\n",
    "    ones = vector of length len(X) where all elements are 1\n",
    "    avg = dot(Xnorm, ones)\n",
    "    return avg\n",
    "```\n",
    "\n",
    "This is great because modern computers can do matrix multiplication more efficiently than for loops. But one last thing remains: getting all the averages of the previous vectors. For this, we can use a triangular matrix.\n",
    "\n",
    "If we have lower triangular matrix $\\mathbf{L} \\in \\mathbb{R}^{m \\times m}$\n",
    "\n",
    "$$\n",
    "\\mathbf{L} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & \\dots & 0 \\\\\n",
    "1 & 1 & 0 & \\dots & 0 \\\\\n",
    "1 & 1 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 1 & 1 & \\dots & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the matrix multiplication $\\mathbf{L} \\mathbf{A}$ is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{L} \\mathbf{A} &= \\begin{bmatrix}\n",
    "1 & 0 & 0 & \\dots & 0 \\\\\n",
    "1 & 1 & 0 & \\dots & 0 \\\\\n",
    "1 & 1 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 1 & 1 & \\dots & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "a_{1 \\, 1} & a_{1 \\, 2} & \\dots & a_{1 \\, k}\\\\\n",
    "a_{2 \\, 1} & a_{2 \\, 2} & \\dots & a_{2 \\, k}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{m \\, 1} & a_{m \\, 2} & \\dots & a_{m \\, k}\\\\\n",
    "\\end{bmatrix} \\\\[35pt]\n",
    "&= \\begin{bmatrix}\n",
    "a_{1 \\, 1} + 0 + \\dots + 0 & a_{1 \\, 2} + 0 + \\dots + 0 & \\dots & a_{1 \\,k} + 0 + \\dots + 0\\\\\n",
    "a_{1 \\, 1} + a_{2 \\, 1} + \\dots + 0 & a_{1 \\, 2} + a_{2 \\, 2} + \\dots + 0 & \\dots & a_{1 \\,k} + a_{2 \\,k} + \\dots + 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{1 \\, 1} + a_{2 \\, 1} + \\dots + a_{m \\, 1} & a_{1 \\, 2} + a_{2 \\, 2} + \\dots + a_{m \\, 2} & \\dots & a_{1 \\,k} + a_{2 \\,k} + \\dots + a_{m \\, k}\\\\\n",
    "\\end{bmatrix} \\\\[35pt]\n",
    "\\mathbf{L} \\mathbf{A} &= \\begin{bmatrix}\n",
    "a_{1 \\, 1} & a_{1 \\, 2} & \\dots & a_{1 \\,k}\\\\\n",
    "a_{1 \\, 1} + a_{2 \\, 1} & a_{1 \\, 2} + a_{2 \\, 2} & \\dots & a_{1 \\,k} + a_{2 \\,k}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{1 \\, 1} + a_{2 \\, 1} + \\dots + a_{m \\, 1} & a_{1 \\, 2} + a_{2 \\, 2} + \\dots + a_{m \\, 2} & \\dots & a_{1 \\,k} + a_{2 \\,k} + \\dots + a_{m \\, k}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we normalize matrix $\\mathbf{A}$ before performing the matrix multiplication with the lower triangular matrix, then each element in the last row is the average of the previous columns. If we want to have correct averages for every row, however, we can normalize the triangular matrix to ensure that each row in the triangular matrix adds to $1$.\n",
    "\n",
    "Let's implement this in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c9935e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "a=tensor([[8., 6., 5.],\n",
      "        [2., 4., 4.],\n",
      "        [7., 4., 5.]])\n",
      "\n",
      "c=tensor([[17., 14., 14.],\n",
      "        [17., 14., 14.],\n",
      "        [17., 14., 14.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0, 10, (3, 3)).float()\n",
    "ones = torch.ones(3, 3)\n",
    "c = ones @ a\n",
    "print(f'{ones=}\\n\\n{a=}\\n\\n{c=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "baf7d001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "a=tensor([[8., 6., 5.],\n",
      "        [2., 4., 4.],\n",
      "        [7., 4., 5.]])\n",
      "\n",
      "c=tensor([[ 8.,  6.,  5.],\n",
      "        [10., 10.,  9.],\n",
      "        [17., 14., 14.]])\n"
     ]
    }
   ],
   "source": [
    "L = torch.tril(torch.ones(3, 3))\n",
    "c = L @ a\n",
    "print(f'{L=}\\n\\n{a=}\\n\\n{c=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ec12aa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "L_norm=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "\n",
      "a=tensor([[8., 6., 5.],\n",
      "        [2., 4., 4.],\n",
      "        [7., 4., 5.]])\n",
      "\n",
      "c=tensor([[8.0000, 6.0000, 5.0000],\n",
      "        [5.0000, 5.0000, 4.5000],\n",
      "        [5.6667, 4.6667, 4.6667]])\n"
     ]
    }
   ],
   "source": [
    "L_norm = L / L.sum(1, keepdim=True)\n",
    "c = L_norm @ a\n",
    "print(f'{L=}\\n\\n{L_norm=}\\n\\n{a=}\\n\\n{c=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9908de",
   "metadata": {},
   "source": [
    "Implementing this method with our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3f60ee6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "83a727f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20dd0d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow[1], xbow2[1], atol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61a58c",
   "metadata": {},
   "source": [
    "### Version 3 - Softmax\n",
    "\n",
    "We can also achieve this using softmax. We start with the lower triangular matrix, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8ea47198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817000c7",
   "metadata": {},
   "source": [
    "Then we have `wei` start as all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ee06ffdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T,  T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83f576",
   "metadata": {},
   "source": [
    "Then every element where `tril` is $0$, we set `wei` to $-\\infin$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73cb95ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542332e",
   "metadata": {},
   "source": [
    "Finally, we perform a softmax. This means we exponentiate each element and take the sum (in this case along dimension 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "71b96e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fa76b",
   "metadata": {},
   "source": [
    "Finally, we perform the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed3465f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e768b834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow3, atol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9091544",
   "metadata": {},
   "source": [
    "This method will be more useful later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35bc41",
   "metadata": {},
   "source": [
    "## Version 4 - Implementing Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0f7caf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2e06d",
   "metadata": {},
   "source": [
    "First, we will implement self-attention with a single head. Each token will emit a **key** vector and a **query** vector.\n",
    "\n",
    "- The key vector asks \"what do I contain?\"\n",
    "\n",
    "- The query vector asks \"what am I looking for?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22cd5c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a6cab447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1470, 0.8530, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2560, 0.6855, 0.0585, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0969, 0.3249, 0.0379, 0.5403, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2202, 0.2055, 0.1001, 0.3504, 0.1239, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0331, 0.1591, 0.2083, 0.3110, 0.0389, 0.2496, 0.0000, 0.0000],\n",
       "        [0.0280, 0.0773, 0.0620, 0.1201, 0.0372, 0.4961, 0.1793, 0.0000],\n",
       "        [0.3269, 0.1925, 0.1365, 0.0435, 0.0895, 0.1349, 0.0112, 0.0650]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "02676d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba7c1f",
   "metadata": {},
   "source": [
    "We also include another vector, the **value** vector, which is the information that token offers to other tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c79ffdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc9301",
   "metadata": {},
   "source": [
    "We also divide by the square root of the head size. This is because, if the key and query tensors have a variance of $1$, then the variance of `wei` will be on the order of head size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b33b2b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key variance: 0.9179, Query variance: 0.9849, Wei variance: 13.9456\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "print(f'Key variance: {k.var().item():.4f}, Query variance: {q.var().item():.4f}, Wei variance: {wei.var().item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781114d8",
   "metadata": {},
   "source": [
    "To fix this, we calculate wei by performing:\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{Q K^T}{\\sqrt{h}}\\right) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83278462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key variance: 0.9812, Query variance: 1.0921, Wei variance: 0.8540\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "print(f'Key variance: {k.var().item():.4f}, Query variance: {q.var().item():.4f}, Wei variance: {wei.var().item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8089e5b4",
   "metadata": {},
   "source": [
    "This is important because `wei` feeds into softmax, which exaggerates very high values (converges to one-hot vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a6f36820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_cuda12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
